{"componentChunkName":"component---src-views-docs-index-tsx","path":"/docs/master","webpackCompilationHash":"aefb7e842b40c338951f","result":{"pageContext":{"isCreatedByStatefulCreatePages":false,"version":"master","versions":{"releases":["0.9"],"prereleases":["1.0"],"branches":["master"]},"content":{"id":"kyma","displayName":"Kyma","description":"Overall documentation for Kyma","type":"root","docs":[{"order":"01-01-in-a-nutshell","title":"In a nutshell","source":"\nKyma allows you to connect applications and third-party services in a cloud-native environment. Use it to create extensions for the existing systems, regardless of the language they are written in. Customize extensions with minimum effort and time devoted to learning their configuration details.\n\nWith Kyma in hand, you can focus purely on coding since it ensures the following out-of-the-box functionalities:\n- Service-to-service communication and proxying (Istio Service Mesh)\n- In-built monitoring, tracing, and logging (Grafana, Prometheus, Jaeger, Loki)\n- Secure authentication and authorization (Dex, Service Identity, TLS, Role Based Access Control)\n- The catalog of services to choose from (Service Catalog, Service Brokers)\n- The development platform to run lightweight functions in a cost-efficient and scalable way (Serverless, Kubeless)\n- The endpoint to register Events and APIs of external applications (Application Connector)\n- The messaging channel to receive Events, enrich them, and trigger business flows using lambdas or services (Event Bus, NATS)\n- CLI supported by the intuitive UI (Console)\n","type":"Overview"},{"order":"01-02-main-features","title":"Main features","source":"\nMajor open-source and cloud-native projects, such as Istio, NATS, Kubeless, and Prometheus, constitute the cornerstone of Kyma. Its uniqueness, however, lies in the \"glue\" that holds these components together. Kyma collects those cutting-edge solutions in one place and combines them with the in-house developed features that allow you to connect and extend your enterprise applications easily and intuitively.\n\nKyma allows you to extend and customize the functionality of your products in a quick and modern way, using serverless computing or microservice architecture. The extensions and customizations you create are decoupled from the core applications, which means that:\n- Deployments are quick.\n- Scaling is independent from the core applications.\n- The changes you make can be easily reverted without causing downtime of the production system.\n\nLast but not least, Kyma is highly cost-efficient. All Kyma native components and the connected open-source tools are written in Go. It ensures low memory consumption and reduced maintenance costs compared to applications written in other programming languages such as Java.\n","type":"Overview"},{"order":"01-03-technology-stack","title":"Technology stack","source":"\nThe entire solution is containerized and runs on a [Kubernetes](https://kubernetes.io/) cluster. Customers can access it easily using a single sign on solution based on the [Dex](https://github.com/coreos/dex) identity provider integrated with any [OpenID Connect](https://openid.net/connect/)-compliant identity provider or a SAML2-based enterprise authentication server.\n\nThe communication between services is handled by the [Istio](https://istio.io/) Service Mesh component which enables security, traffic management, routing, resilience (retry, circuit breaker, timeouts), monitoring, and tracing without the need to change the application code.\nBuild your applications using services provisioned by one of the many Service Brokers compatible with the [Open Service Broker API](https://www.openservicebrokerapi.org/), and monitor the speed and efficiency of your solutions using [Prometheus](https://prometheus.io/), which gives you the most accurate and up-to-date monitoring data.\n","type":"Overview"},{"order":"01-04-key-components","title":"Key components","source":"\nKyma is built of numerous components but these three drive it forward:\n\n  - **Application Connector**:\n    - Simplifies and secures the connection between external systems and Kyma\n    - Registers external Events and APIs in the Service Catalog and simplifies the API usage\n    - Provides asynchronous communication with services and lambdas deployed in Kyma through Events\n    - Manages secure access to external systems\n    - Provides monitoring and tracing capabilities to facilitate operational aspects\n  - **Serverless**: \n    - Ensures quick deployments following a lambda function approach\n    - Enables scaling independent of the core applications\n    - Gives a possibility to revert changes without causing production system downtime\n    - Supports the complete asynchronous programming model\n    - Offers loose coupling of Event providers and consumers\n    - Enables flexible application scalability and availability\n  - **Service Catalog**:\n    - Connects services from external sources\n    - Unifies the consumption of internal and external services thanks to compliance with the Open Service Broker standard\n    - Provides a standardized approach to managing the API consumption and access\n    - Eases the development effort by providing a catalog of API and Event documentation to support automatic client code generation\n  \n\nThis basic use case shows how the three components work together in Kyma:\n\n![key-components](./assets/ac-s-sc.svg)\n","type":"Overview"},{"order":"01-05-kyma-and-knative","title":"Kyma and Knative - brothers in arms","source":"\nIntegration with Knative is a step towards Kyma modularization and the \"slimming\" approach which aims to extract some out-of-the-box components and provide you with a more flexible choice of tools to use in Kyma.\n\nBoth Kyma and Knative are Kubernetes and Istio-based systems that offer development and eventing platforms. The main difference, however, is their focus. While Knative concentrates more on providing the building blocks for running serverless workloads, Kyma focuses on integrating those blocks with external services and applications.\n\nThe diagram shows dependencies between the components:\n\n![kyma-knative](./assets/kyma-knative.svg)\n\nKyma and Knative cooperation focuses on replacing Kyma eventing with Knative eventing, and Kyma Serverless with Knative serving.\n","type":"Overview"},{"order":"01-06-how-to-start","title":"How to start","source":"\nMinikube allows you to run Kyma locally, develop, and test your solutions on a small scale before you push them to a cluster. With the Installation and Getting Started guides at hand, you can start developing in a matter of minutes.\n\nRead, learn, and try on your own to:\n\n- [Install Kyma locally](#installation-install-kyma-locally)\n- [Install Kyma on a cluster](#installation-install-kyma-on-a-cluster)\n- [Deploy a sample service locally](#tutorials-sample-service-deployment-on-local)\n- [Deploy a service on a cluster](#tutorials-sample-service-deployment-on-a-cluster)\n- [Develop a service locally without using Docker](#tutorials-develop-a-service-locally-without-using-docker)\n- [Publish a service Docker image and deploy it to Kyma](#tutorials-publish-a-service-docker-image-and-deploy-it-to-kyma)\n- [Configure the Installer with override values for Helm charts](#tutorials-helm-overrides-for-kyma-installation)\n- [Register a Broker in Service Catalog](/docs/master/components/service-catalog#tutorials-register-a-broker-in-the-service-catalog)\n- [Create a new Application](/docs/master/components/application-connector#tutorials-create-a-new-application)\n- [Get the client certificate](/docs/master/components/application-connector#tutorials-get-the-client-certificate)\n- [Register a service](/docs/master/components/application-connector#tutorials-register-a-service)\n- [Bind an Application to a Namespace](/docs/master/components/application-connector#tutorials-bind-an-application-to-a-namespace)\n- [Trigger a lambda with events](/docs/master/components/application-connector#tutorials-trigger-a-lambda-with-events)\n- [Call a registered external service from Kyma](/docs/master/components/application-connector#tutorials-call-a-registered-external-service-from-kyma)\n- [Expose custom metrics in Kyma](/docs/master/components/monitoring#tutorials-expose-custom-metrics-in-kyma)\n","type":"Overview"},{"order":"03-01-components","title":"Components","source":"\nKyma is built on the foundation of the best and most advanced open-source projects which make up the components readily available for customers to use.\nThis section describes the Kyma components.\n\n## Service Catalog\n\nThe Service Catalog lists all of the services available to Kyma users through the registered [Service Brokers](/docs/master/components/service-catalog/#service-brokers-service-brokers). Use the Service Catalog to provision new services in the\nKyma [Kubernetes](https://kubernetes.io/) cluster and create bindings between the provisioned service and an application.\n\n\n## Service Mesh\n\nThe Service Mesh is an infrastructure layer that handles service-to-service communication, proxying, service discovery, traceability, and security independent of the code of the services. Kyma uses the [Istio](https://istio.io/) Service Mesh that is customized for the specific needs of the implementation.\n\n## Security\n\nKyma security enforces RBAC (Role Based Access Control) in the cluster. [Dex](https://github.com/coreos/dex) handles the identity management and identity provider integration. It allows you to integrate any [OpenID Connect](https://openid.net/connect/) or SAML2-compliant identity provider with Kyma using [connectors](https://github.com/coreos/dex#connectors). Additionally, Dex provides a static user store which gives you more flexibility when managing access to your cluster.   \n\n## Helm Broker\n\nThe Helm Broker is a Service Broker which runs in the Kyma cluster and deploys Kubernetes native resources using [Helm](https://github.com/kubernetes/helm) and Kyma bundles. A bundle is an abstraction layer over a Helm chart which allows you to represent it as a ClusterServiceClass in the Service Catalog. Use bundles to install the [GCP Broker](/docs/master/components/service-catalog#service-brokers-gcp-broker) and the [Azure Service Broker](/docs/master/components/service-catalog#service-brokers-azure-service-broker) in Kyma.\n\n## Application Connector\n\nThe Application Connector is a proprietary Kyma solution. This endpoint is the Kyma side of the connection between Kyma and the external solutions. The Application Connector allows you to register the APIs and the Event Catalog, which lists all of the available events, of the connected solution. Additionally, the Application Connector proxies the calls from Kyma to external APIs in a secure way.\n\n## Event Bus\n\nKyma Event Bus receives Events from external solutions and triggers the business logic created with lambda functions and services in Kyma. The Event Bus is based on the [NATS Streaming](https://nats.io/) open source messaging system for cloud-native applications.\n\n## Serverless\n\nThe Kyma Serverless component allows you to reduce the implementation and operation effort of an application to the absolute minimum. Kyma Serverless provides a platform to run lightweight functions in a cost-efficient and scalable way using JavaScript and Node.js. Kyma Serverless is built on the [Kubeless](http://kubeless.io/) framework, which allows you to deploy lambda functions,\nand uses the [NATS](https://nats.io/) messaging system that monitors business events and triggers functions accordingly.  \n\n## Monitoring\n\nKyma comes bundled with tools that give you the most accurate and up-to-date monitoring data. [Prometheus](https://prometheus.io/) open source monitoring and alerting toolkit provides this data, which is consumed by different add-ons, including [Grafana](https://grafana.com/) for analytics and monitoring, and [Alertmanager](https://prometheus.io/docs/alerting/alertmanager/) for handling alerts.\n\n## Tracing\n\nThe tracing in Kyma uses the [Jaeger](https://github.com/jaegertracing) distributed tracing system. Use it to analyze performance by scrutinizing the path of the requests sent to and from your service. This information helps you optimize the latency and performance of your solution.\n\n## Logging\n\nLogging in Kyma uses [Loki](https://github.com/grafana/loki), a Prometheus-like log management system.\n","type":"Details"},{"order":"03-02-namespaces","title":"Namespaces","source":"\nA Namespace is a security and organizational unit which allows you to divide the cluster into smaller units to use for different purposes, such as development and testing.\n\nNamespaces available for users are marked with the `env: \"true\"` label. The Kyma UI only displays the Namespaces marked with the `env: \"true\"` label.\n\n\n## Default Kyma Namespaces\n\nKyma comes configured with default Namespaces dedicated for system-related purposes. The user cannot modify or remove any of these Namespaces.\n\n- `kyma-system` - This Namespace contains all of the Kyma Core components.\n- `kyma-integration` - This Namespace contains all of the Application Connector components responsible for the integration of Kyma and external solutions.\n- `kyma-installer` - This Namespace contains all of the Kyma Installer components, objects, and Secrets.\n- `istio-system` - This Namespace contains all of the Istio-related components.\n\n## Namespaces for users in Kyma\n\nKyma comes with three Namespaces ready for you to use.\n- `production`\n- `qa`\n- `stage`\n\n### Create a new Namespace for users\n\nCreate a Namespace and mark it with the `env: \"true\"` label to make it available for Kyma users. Use this command to do that in a single step:\n\n```\n$ cat <<EOF | kubectl create -f -\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: my-namespace\n  labels:\n    env: \"true\"\nEOF\n```\n\nInitially, the system deploys two template roles: `kyma-reader-role` and `kyma-admin-role`. The controller finds the template roles by filtering the roles available in the `kyma-system` Namespace by the label `env: \"true\"`. The controller copies these roles into the newly created Namespace.\n","type":"Details"},{"order":"03-03-testing","title":"Testing Kyma","source":"\nKyma components use [Octopus](http://github.com/kyma-incubator/octopus) for testing. \nOctopus is a testing framework that allows you to run tests defined as Docker images on a running cluster.\nOctopus uses two CustomResourceDefinitions (CRDs):\n- TestDefinition, which defines your test as a Pod specification.\n- ClusterTestSuite, which defines a suite of tests to execute and how to execute them.\n\n## Add a new test\nTo add a new test, create a `yaml` file with TestDefinition CR in your chart. To comply with the convention, place it under the `tests` directory.\nSee the exemplary chart structure for Dex:\n\n```\n# Chart tree\ndex\n├── Chart.yaml\n├── README.md\n├── templates\n│   ├── tests\n│   │   └── test-dex-connection.yaml\n│   ├── dex-deployment.yaml\n│   ├── dex-ingress.yaml\n│   ├── dex-rbac-role.yaml\n│   ├── dex-service.yaml\n│   ├── pre-install-dex-account.yaml\n│   ├── pre-install-dex-config-map.yaml\n│   └── pre-install-dex-secrets.yaml\n└── values.yaml\n```\n\nThe test adds a new **test-dex-connection.yaml** under the `templates/tests` directory.\nFor more information on TestDefinition, read the [Octopus documentation](https://github.com/kyma-incubator/octopus/blob/master/docs/crd-test-definition.md).\n\nThe following example presents TestDefinition with a container that calls the Dex endpoint with cURL. You must define at least the **spec.template** parameter which is of the `PodTemplateSpec` type.\n\n```yaml\napiVersion: \"testing.kyma-project.io/v1alpha1\"\nkind: TestDefinition\nmetadata:\n  name: \"test-{{ template \"fullname\" . }}-connection-dex\"\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: \"false\"\n    spec:\n      containers:\n      - name: \"test-{{ template \"fullname\" . }}-connection-dex\"\n        image: tutum/curl:alpine\n        command: [\"/usr/bin/curl\"]\n        args: [\n          \"--fail\",\n          \"--max-time\", \"10\",\n          \"--retry\", \"60\",\n          \"--retry-delay\", \"3\",\n          \"http://dex-service.{{ .Release.Namespace }}.svc.cluster.local:5556/.well-known/openid-configuration\"\n        ]\n      restartPolicy: Never\n\n```\n\n## Tests execution\nTo run all tests, use the `testing.sh` script located in the `/installation/scripts/` directory. \nInternally, the ClusterTestSuite resource is defined. It fetches all TestDefinitions and executes them.\n\n\n### Run tests manually\nTo run tests manually, create your own ClusterTestSuite resource. See the following example:\n\n```yaml\napiVersion: testing.kyma-project.io/v1alpha1\nkind: ClusterTestSuite\nmetadata:\n  labels:\n    controller-tools.k8s.io: \"1.0\"\n  name: {my-suite}\nspec:\n  maxRetries: 0\n  concurrency: 1\n  count: 1\n```\n\nCreation of the suite triggers tests execution. See the current tests progress in the ClusterTestSuite status. Run:\n```bash\n kubectl get cts {my-suite} -oyaml\n ```\n \nThe sample output looks as follows:\n```\napiVersion: testing.kyma-project.io/v1alpha1\nkind: ClusterTestSuite\nmetadata:\n  name: {my-suite}\nspec:\n  concurrency: 1\n  count: 1\n  maxRetries: 0\nstatus:\n  conditions:\n  - status: \"True\"\n    type: Running\n  results:\n  - executions:\n    - completionTime: 2019-04-05T12:23:00Z\n      id: {my-suite}-test-dex-dex-connection-dex-0\n      podPhase: Succeeded\n      startTime: 2019-04-05T12:22:54Z\n    name: test-dex-dex-connection-dex\n    namespace: kyma-system\n    status: Succeeded\n  - executions:\n    - id: {my-suite}-test-core-core-ui-acceptance-0\n      podPhase: Running\n      startTime: 2019-04-05T12:37:53Z\n    name: test-core-core-ui-acceptance\n    namespace: kyma-system\n    status: Running\n  - executions: []\n    name: test-api-controller-acceptance\n    namespace: kyma-system\n    status: NotYetScheduled\n  startTime: 2019-04-05T12:22:53Z\n```\n\nThe ID of the test execution is the same as the ID of the testing Pod. The testing Pod is created in the same Namespace as its TestDefinition. To get logs for a specific test, run the following command:\n```\nkubectl logs {execution-id} -n {test-def-namespace}\n```","type":"Details"},{"order":"03-04-charts","title":"Charts","source":"\nKyma uses Helm charts to deliver single components and extensions, as well as the core components. This document contains information about the chart-related technical concepts, dependency management to use with Helm charts, and chart examples.\n\n## Manage dependencies with Init Containers\n\nThe **ADR 003: Init Containers for dependency management** document declares the use of Init Containers as the primary dependency mechanism.\n\n[Init Containers](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) present a set of distinctive behaviors:\n\n* They always run to completion.\n* They start sequentially, only after the preceding Init Container completes successfully.\n  If any of the Init Containers fails, the Pod restarts. This is always true, unless the `restartPolicy` equals `never`.\n\n[Readiness Probes](https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#container-probes) ensure that the essential containers are ready to handle requests before you expose them. At a minimum, probes are defined for every container accessible from outside of the Pod. It is recommended to pair the Init Containers with readiness probes to provide a basic dependency management solution.\n\n## Examples\n\nHere are some examples:\n\n1. Generic\n\n\n```yaml\napiVersion: apps/v1beta2\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 1\n```\n\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\nspec:\n  initContainers:\n  - name: init-myservice\n    image: busybox\n    command: ['sh', '-c', 'until nslookup nginx; do echo waiting for nginx; sleep 2; done;']\n  containers:\n  - name: myapp-container\n    image: busybox\n    command: ['sh', '-c', 'echo The app is running! && sleep 3600']\n```\n\n2. Kyma\n\n```yaml\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: helm-broker\n  labels:\n    app: helm-broker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: helm-broker\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: helm-broker\n    spec:\n\n      initContainers:\n      - name: init-helm-broker\n        image: eu.gcr.io/kyma-project/alpine-net:0.2.74\n        command: ['sh', '-c', 'until nc -zv service-catalog-controller-manager.kyma-system.svc.cluster.local 8080; do echo waiting for etcd service; sleep 2; done;']\n\n      containers:\n      - name: helm-broker\n        ports:\n        - containerPort: 6699\n        readinessProbe:\n          tcpSocket:\n            port: 6699\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          successThreshold: 1\n          timeoutSeconds: 2\n```\n\n## Support for the Helm wait flag\n\nHigh level Kyma components, such as **core**, come as Helm charts. These charts are installed as part of a single Helm release. To provide ordering for these core components, the Helm client runs with the `--wait` flag. As a result, Tiller waits for the readiness of all of the components, and then evaluates the readiness.\n\nFor `Deployments`, set the strategy to `RollingUpdate` and set the `MaxUnavailable` value to a number lower than the number of replicas. This setting is necessary, as readiness in Helm v2.10.0 is fulfilled if the number of replicas in ready state is not lower than the expected number of replicas:\n\n```\nReadyReplicas >= TotalReplicas - MaxUnavailable\n```\n\n## Chart installation details\n\nThe Tiller server performs the chart installation process. This is the order of operations that happen during the chart installation:\n\n* resolve values\n* recursively gather all templates with the corresponding values\n* sort all templates\n* render all templates\n* separate hooks and manifests from files into sorted lists\n* aggregate all valid manifests from all sub-charts into a single manifest file\n* execute PreInstall hooks\n* create a release using the ReleaseModule API and, if requested, wait for the actual readiness of the resources\n* execute PostInstall hooks\n\n## Notes\n\nAll notes are based on Helm v2.10.0 implementation and are subject to change in future releases.\n\n* Regardless of how complex a chart is, and regardless of the number of sub-charts it references or consists of, it's always evaluated as one. This means that each Helm release is compiled into a single Kubernetes manifest file when applied on API server.\n\n* Hooks are parsed in the same order as manifest files and returned as a single, global list for the entire chart. For each hook the weight is calculated as a part of this sort.\n\n* Manifests are sorted by `Kind`. You can find the list and the order of the resources on the Kubernetes [Tiller](https://github.com/kubernetes/helm/blob/v2.10.0/pkg/tiller/kind_sorter.go#L29) website.\n\n## Glossary\n\n* **resource** is any document in a chart recognized by Helm or Tiller. This includes manifests, hooks, and notes.\n* **template** is a valid Go template. Many of the resources are also Go templates.\n","type":"Details"},{"order":"03-05-deploy-private-registry","title":"Deploy with a private Docker registry","source":"\nDocker is a free tool to deploy applications and servers. To run an application on Kyma, provide the application binary file as a Docker image located in a Docker registry. Use the `DockerHub` public registry to upload your Docker images for free access to the public. Use a private Docker registry to ensure privacy, increased security, and better availability.\n\nThis document shows how to deploy a Docker image from your private Docker registry to the Kyma cluster.\n\n## Details\n\nThe deployment to Kyma from a private registry differs from the deployment from a public registry. You must provide Secrets accessible in Kyma, and referenced in the `.yaml` deployment file. This section describes how to deploy an image from a private Docker registry to Kyma. Follow the deployment steps:\n\n1. Create a Secret resource.\n2. Write your deployment file.\n3. Submit the file to the Kyma cluster.\n\n### Create a Secret for your private registry\n\nA Secret resource passes your Docker registry credentials to the Kyma cluster in an encrypted form. For more information on Secrets, refer to the [Kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/secret/).\n\nTo create a Secret resource for your Docker registry, run the following command:\n\n```bash\nkubectl create secret docker-registry {secret-name} --docker-server={registry FQN} --docker-username={user-name} --docker-password={password} --docker-email={registry-email} --namespace={namespace}  \n```\n\nRefer to the following example:\n```bash\nkubectl create secret docker-registry docker-registry-secret --docker-server=myregistry:5000 --docker-username=root --docker-password=password --docker-email=example@github.com --namespace=production\n```\n\nThe Secret is associated with a specific Namespace. In the example, the Namespace is `production`. However, you can modify the Secret to point to any desired Namespace.\n\n### Write your deployment file\n\n1. Create the deployment file with the `.yml` extension and name it `deployment.yml`.\n\n2. Describe your deployment in the `.yml` file. Refer to the following example:\n\n```yaml\napiVersion: apps/v1beta2\nkind: Deployment\nmetadata:\n  namespace: production # {production/stage/qa}\n  name: my-deployment # Specify the deployment name.\n  annotations:\n    sidecar.istio.io/inject: true\nspec:\n  replicas: 3 # Specify your replica - how many instances you want from that deployment.\n  selector:\n    matchLabels:\n      app: app-name # Specify the app label. It is optional but it is a good practice.\n  template:\n    metadata:\n      labels:\n        app: app-name # Specify app label. It is optional but it is a good practice.\n        version: v1 # Specify your version.\n    spec:\n      containers:\n      - name: container-name # Specify a meaningful container name.\n        image: myregistry:5000/user-name/image-name:latest # Specify your image {registry FQN/your-username/your-space/image-name:image-version}.\n        ports:\n          - containerPort: 80 # Specify the port to your image.\n      imagePullSecrets:\n        - name: docker-registry-secret # Specify the same Secret name you generated in the previous step for this Namespace.\n        - name: example-secret-name # Specify your Namespace Secret, named `example-secret-name`.\n\n```\n3. Submit you deployment file using this command:\n\n```bash\nkubectl apply -f deployment.yml\n```\nYour deployment is now running on the Kyma cluster.\n","type":"Details"},{"order":"04-01-overview","title":"Overview","source":"\nKyma is a complex tool which consists of many different [components](#details-components) that provide various functionalities to extend your application. This entails high technical requirements that can influence your local development process. To meet the customer needs, we ensured Kyma modularity. This way you can decide not to include a given component in the Kyma installation, or install it after the Kyma installation process.\n\nTo make the local development process easier, we introduced the **Kyma Lite** concept in which case some components are not included in the local installation process by default. These are the Kyma and Kyma Lite components:\n\n| Component | Kyma | Kyma Lite |\n|----------------|------|------|\n| `core` | ✅ | ✅ |\n| `cms` | ✅ | ✅ |\n| `cluster-essentials` | ✅ | ✅ |\n| `application-connector` | ✅ | ✅ |\n| `ark` | ✅ | ⛔️ |\n| `assetstore` | ✅ | ✅ |\n| `dex` | ✅ | ✅ |\n| `helm-broker` | ✅ | ✅ |\n| `istio` | ✅ | ✅ |\n| `istio-kyma-patch` | ✅ | ✅ |\n| `jaeger` | ✅ | ⛔️ |\n| `logging` | ✅ | ⛔️ |\n| `monitoring` | ✅ | ⛔️ |\n| `prometheus-operator` | ✅ | ⛔️ |\n| `service-catalog` | ✅ | ✅ |\n| `service-catalog-addons` | ✅ | ✅ |\n| `nats-streaming` | ✅ | ✅ |\n\n## Installation guides\n\nFollow these installation guides to install Kyma locally or on a cluster:\n\n- [Install Kyma locally](#installation-install-kyma-locally)\n- [Install Kyma on a cluster](#installation-install-kyma-on-a-cluster)\n\nRead rest of the installation documents to learn how to:\n- [Disable the selected components' installation or install them separately](#installation-custom-component-installation)\n- [Update Kyma](#installation-update-kyma)\n- [Reinstall Kyma](#installation-reinstall-kyma)\n- [Get in-depth knowledge about the installation scripts](#installation-local-installation-scripts-deep-dive)\n\n>**NOTE:** Make sure to check whether the version of the documentation in the left pane of the `kyma-project.io` is compatible with your Kyma version.\n","type":"Installation"},{"order":"04-02-local-installation","title":"Install Kyma locally","source":"\nThis Installation guide shows you how to quickly deploy Kyma locally on the MacOS, Linux, and Windows platforms. Kyma is installed locally using a proprietary installer based on a [Kubernetes operator](https://coreos.com/operators/).\n\n>**TIP:** See [this](#troubleshooting-overview) document for troubleshooting tips.\n\n## Prerequisites\n\n- [Kyma CLI](https://github.com/kyma-project/cli)\n- [Docker](https://www.docker.com/get-started)\n- [Minikube](https://github.com/kubernetes/minikube) 1.0\n- [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) 1.13\n- [Helm](https://github.com/kubernetes/helm) 2.10\n\nVirtualization:\n\n- [Hyperkit driver](https://minikube.sigs.k8s.io/docs/reference/drivers/hyperkit/) - MacOS only\n- [VirtualBox](https://www.virtualbox.org/) - Linux only\n\n> **NOTE**: To work with Kyma, use only the provided commands. Kyma requires a specific Minikube configuration and does not work on a basic Minikube cluster that you can start using the `minikube start` command.\n\n\n## Install Kyma\n\nFollow these instructions to install Kyma from a release or from sources:\n<div tabs name=\"prerequisites\">\n  <details>\n  <summary>\n  From a release\n  </summary>\n\n  1. Provision a Kubernetes cluster on Minikube. Run:\n\n     ```bash\n     kyma provision minikube\n     ```\n     >**NOTE:** The `provision` command uses the default Minikube VM driver installed for your operating system. For a list of supported VM drivers see [this document](https://kubernetes.io/docs/setup/minikube/#quickstart).\n\n  2. Install the latest Kyma release on Minikube:\n     ```bash\n     kyma install\n     ```\n     >**NOTE** If you want to install a specific release version, go to the [GitHub releases page](https://github.com/kyma-project/kyma/releases) to find out more about available releases. Use the release version as a parameter when calling ` kyma install --release {KYMA_RELEASE}`.\n\n  </details>\n  <details>\n  <summary>\n  From sources\n  </summary>\n\n  1. Open a terminal window and navigate to a space in which you want to store local Kyma sources.\n\n  2. Clone the `Kyma` repository using HTTPS. Run:\n\n     ```bash\n     git clone https://github.com/kyma-project/kyma.git\n     ```\n  3. Provision a Kubernetes cluster on Minikube. Run:\n     ```bash\n     kyma provision minikube\n     ```\n     >**NOTE:** The `provision` command uses default Minikube VM driver installed for your OS. For a list of supported VM drivers see [this document](http://github.com/kyma-project/cli).\n\n  4. Install Kyma from sources. Run:\n\n     ```bash\n     kyma install --local --src-path {YOUR_KYMA_SOURCE_PATH}\n     ```\n\n   </details>\n</div>\n\n## Post-installation steps\n\nKyma comes with a local wildcard self-signed `server.crt` certificate. The `kyma install` command downloads and adds this certificate to the trusted certificates in your OS so you can access the Console UI.\n\n>**NOTE:** Mozilla Firefox uses its own certificate keychain. If you want to access the Console UI though Firefox, add the Kyma wildcard certificate to the certificate keychain of the browser. To access the Application Connector and connect an external solution to the local deployment of Kyma, you must add the certificate to the trusted certificate storage of your programming environment. Read [this](/docs/master/components/application-connector#details-access-the-application-connector-on-a-local-kyma-deployment) document to learn more.\n\n1. After the installation is completed, you can access the Console UI. Go to [this](https://console.kyma.local) address and select **Login with Email**. Use the **admin@kyma.cx** email address and the password printed in the terminal once the installation process is completed.\n\n2. At this point, Kyma is ready for you to explore. See what you can achieve using the Console UI or check out one of the [available examples](https://github.com/kyma-project/examples).\n\nRead [this](#installation-reinstall-kyma) document to learn how to reinstall Kyma without deleting the cluster from Minikube.\nTo learn how to test Kyma, see [this](#details-testing-kyma) document.\n\n## Stop and restart Kyma without reinstalling\n\nUse the Kyma CLI to restart the Minikube cluster without reinstalling Kyma. Follow these steps to stop and restart your cluster:\n\n1. Stop the Minikube cluster with Kyma installed. Run:\n   ```\n   minikube stop\n   ```\n2. Restart the cluster without reinstalling Kyma. Run:\n   ```bash\n   kyma provision minikube\n   ```\n\nThe Kyma CLI discovers that a Minikube cluster is initialized and asks if you want to delete it. Answering `no` causes the Kyma CLI to start the Minikube cluster and restarts all of the previously installed components. Even though this procedure takes some time, it is faster than a clean installation as you don't download all of the required Docker images.\n\nTo verify that the restart is successful, run this command and check if all Pods have the `RUNNING` status:\n\n```\nkubectl get pods --all-namespaces\n```","type":"Installation"},{"order":"04-03-cluster-installation","title":"Install Kyma on a cluster","source":"\nThis installation guide explains how you can quickly deploy Kyma on a cluster with a wildcard DNS provided by [`xip.io`](http://xip.io) using a GitHub release of your choice.\n\n>**NOTE:** A xip.io domain is not recomended for production. If you want to expose the Kyma cluster on your own domain, follow [this](#installation-use-your-own-domain) installation guide. To install using your own image instead of a GitHub release, follow [these](#installation-use-your-own-kyma-installer-image) instructions.\n\n## Prerequisites\n\n<div tabs name=\"prerequisites\" group=\"cluster-installation\">\n  <details>\n  <summary label=\"gke\">\n  GKE\n  </summary>\n  \n- [Google Cloud Platform](https://console.cloud.google.com/) (GCP) project with Kubernetes Engine API enabled\n- [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) 1.14.6 or higher\n- [gcloud](https://cloud.google.com/sdk/gcloud/)\n\n>**NOTE:** Running Kyma on GKE requires three [`n1-standard-4` machines](https://cloud.google.com/compute/docs/machine-types). You create these machines when you complete the **Prepare the GKE cluster** step.\n\n  </details>\n  <details>\n  <summary label=\"aks\">\n  AKS\n  </summary>\n\n- [Microsoft Azure](https://azure.microsoft.com) account\n- [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) 1.14.6 or higher\n- [Azure CLI](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli)\n\n>**NOTE:** Running Kyma on AKS requires three [`Standard_D4_v3` machines](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-general#dv3-series-1). You create these machines when you complete the **Prepare the AKS cluster** step.\n\n>**CAUTION:** If you define your own Kubernetes jobs on the AKS cluster, follow [this](/docs/master/components/service-mesh/#troubleshooting-kubernetes-jobs-fail-on-aks) troubleshooting guide to avoid jobs running endlessly on AKS deployments of Kyma.\n\n  </details>\n  <details>\n  <summary label=\"gardener\">\n  Gardener\n  </summary>\n\n- [Gardener](https://gardener.cloud/) seed cluster\n- [Google Cloud Platform](https://console.cloud.google.com/) (GCP) project with Kubernetes Engine API enabled or a [Microsoft Azure](https://azure.microsoft.com) account\n- [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) 1.14.6 or higher\n\n  </details>\n  <details>\n  <summary label=\"lol\">\n  LOL\n  </summary>\n\n    dupa\n\n  </details>\n</div>\n\n## Choose the release to install\n\n1. Go to [this](https://github.com/kyma-project/kyma/releases/) page and choose the release you want to install.\n\n2. Export the release version as an environment variable. Run:\n\n    ```bash\n    export KYMA_VERSION={KYMA_RELEASE_VERSION}\n    ```\n\n## Prepare the cluster\n\n<div tabs name=\"prepare-cluster\" group=\"cluster-installation\">\n  <details>\n  <summary label=\"gke\">\n  GKE\n  </summary>\n  \n1. Select a name for your cluster. Export the cluster name, the name of your GCP project, and the [zone](https://cloud.google.com/compute/docs/regions-zones/) you want to deploy to as environment variables. Run:\n\n    ```bash\n    export CLUSTER_NAME={CLUSTER_NAME_YOU_WANT}\n    export GCP_PROJECT={YOUR_GCP_PROJECT}\n    export GCP_ZONE={GCP_ZONE_TO_DEPLOY_TO}\n    ```\n\n2. Create a cluster in the zone defined in the previous step. Run:\n\n    ```bash\n    gcloud container --project \"$GCP_PROJECT\" clusters \\\n    create \"$CLUSTER_NAME\" --zone \"$GCP_ZONE\" \\\n    --cluster-version \"1.14\" --machine-type \"n1-standard-4\" \\\n    --addons HorizontalPodAutoscaling,HttpLoadBalancing\n    ```\n\n3. Configure kubectl to use your new cluster. Run:\n\n    ```bash\n    gcloud container clusters get-credentials $CLUSTER_NAME --zone $GCP_ZONE --project $GCP_PROJECT\n    ```\n\n4. Add your account as the cluster administrator:\n\n    ```bash\n    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account)\n    ```\n  \n  </details>\n  <details>\n  <summary label=\"aks\">\n  AKS\n  </summary>\n\n1. Select a name for your cluster. Set the cluster name, the resource group and region as environment variables. Run:\n\n    ```bash\n    export RS_GROUP={YOUR_RESOURCE_GROUP_NAME}\n    export CLUSTER_NAME={YOUR_CLUSTER_NAME}\n    export REGION={YOUR_REGION} #westeurope\n    ```\n\n2. Create a resource group that will contain all your resources:\n\n    ```bash\n    az group create --name $RS_GROUP --location $REGION\n    ```\n\n3. Create an AKS cluster. Run:\n\n    ```bash\n    az aks create \\\n      --resource-group $RS_GROUP \\\n      --name $CLUSTER_NAME \\\n      --node-vm-size \"Standard_D4_v3\" \\\n      --kubernetes-version 1.14.6 \\\n      --enable-addons \"monitoring,http_application_routing\" \\\n      --generate-ssh-keys\n    ```\n\n4. To configure kubectl to use your new cluster, run:\n\n    ```bash\n    az aks get-credentials --resource-group $RS_GROUP --name $CLUSTER_NAME\n    ```\n\n5. Add additional privileges to be able to access readiness probes endpoints on your AKS cluster.\n\n    ```bash\n    kubectl apply -f https://raw.githubusercontent.com/kyma-project/kyma/$KYMA_VERSION/installation/resources/azure-crb-for-healthz.yaml\n    ```\n\n6. Install custom installation overrides for AKS. Run:\n\n    ```bash\n    kubectl create namespace kyma-installer \\\n    && kubectl create configmap aks-overrides -n kyma-installer --from-literal=global.proxy.excludeIPRanges=10.0.0.1 \\\n    && kubectl label configmap aks-overrides -n kyma-installer installer=overrides component=istio\n    ```\n\n    >**TIP:** An example config map is available [here](./assets/aks-overrides.yaml).\n\n  </details>\n  <details>\n  <summary label=\"gardener\">\n  Gardener\n  </summary>\n  \n1. In the left navigation of the Gardener UI, go to the **Secrets** tab and add Secrets to enable provisioning clusters on different architectures. To learn about the requirements for each environment, click the question mark buttons.\n\n2. Provision a cluster form the **Clusters** tab. Click the plus sign in the lower-right corner and choose the infrastructure in which you want to provision your cluster. Apply these settings in the following tabs:\n\n   | Tab  |  Setting |  Required value |\n   |---|---|---|\n   | Infrastructure |  Kubernetes | `1.14.6`  |\n   | Worker  |  Machine type | `n1-standard-4` (GCP) `Standard_D4_v3` (Azure) |\n   | Worker  | Autoscaler min.  | `3` |\n\n3. After you provision the cluster, download the kubeconfig file available under the **Show Cluster Access** option in the **Actions** column.\n\n4. Export the downloaded kubeconfig as an environment variable to connect to the cluster you provisioned. Run:\n\n    ```bash\n    export KUBECONFIG={PATH_TO_KUBECONFIG_FILE}\n    ```\n\n>**NOTE:** On an Azure cluster, make sure to run all commands from steps 5 and 6 of [this](#installation-install-kyma-on-a-cluster--prepare-cluster--aks) section.\n\n  </details>\n</div>\n\n## Install Kyma\n\n1. Install Tiller on the cluster you provisioned. Run:\n\n   ```bash\n   kubectl apply -f https://raw.githubusercontent.com/kyma-project/kyma/$KYMA_VERSION/installation/resources/tiller.yaml\n   ```\n\n2. Deploy Kyma. Run:\n\n    ```bash\n    kubectl apply -f https://github.com/kyma-project/kyma/releases/download/$KYMA_VERSION/kyma-installer-cluster.yaml\n    ```\n\n3. Check if the Pods of Tiller and the Kyma Installer are running:\n\n    ```bash\n    kubectl get pods --all-namespaces\n    ```\n\n4. To watch the installation progress, run:\n\n    ```bash\n    while true; do \\\n      kubectl -n default get installation/kyma-installation -o jsonpath=\"{'Status: '}{.status.state}{', description: '}{.status.description}\"; echo; \\\n      sleep 5; \\\n    done\n    ```\n\nAfter the installation process is finished, the `Status: Installed, description: Kyma installed` message appears.\n\nIf you receive an error, fetch the Kyma Installer logs using this command:\n\n```bash\nkubectl -n kyma-installer logs -l 'name=kyma-installer'\n```\n\n## Post-installation steps\n\n### Add the xip.io self-signed certificate to your OS trusted certificates\n\nAfter the installation, add the custom Kyma [`xip.io`](http://xip.io/) self-signed certificate to the trusted certificates of your OS. For MacOS, run:\n\n```bash\n  tmpfile=$(mktemp /tmp/temp-cert.XXXXXX) \\\n  && kubectl get configmap net-global-overrides -n kyma-installer -o jsonpath='{.data.global\\.ingress\\.tlsCrt}' | base64 --decode > $tmpfile \\\n  && sudo security add-trusted-cert -d -r trustRoot -k /Library/Keychains/System.keychain $tmpfile \\\n  && rm $tmpfile\n  ```\n\n### Access the cluster\n\n1. To get the address of the cluster's Console, check the host of the Console's virtual service. The name of the host of this virtual service corresponds to the Console URL. To get the virtual service host, run:\n\n    ```bash\n    kubectl get virtualservice core-console -n kyma-system -o jsonpath='{ .spec.hosts[0] }'\n    ```\n\n2. Access your cluster under this address:\n\n    ```bash\n    https://{VIRTUAL_SERVICE_HOST}\n    ```\n\n3. To log in to your cluster's Console UI, use the default `admin` static user. Click **Login with Email** and sign in with the **admin@kyma.cx** email address. Use the password contained in the `admin-user` Secret located in the `kyma-system` Namespace. To get the password, run:\n\n    ```bash\n    kubectl get secret admin-user -n kyma-system -o jsonpath=\"{.data.password}\" | base64 --decode\n    ```\n\nIf you need to use Helm and access Tiller, complete the additional configuration after the installation.If you need to use Helm and access Tiller, complete the [additional configuration](#installation-use-helm) after the installation.","type":"Installation"},{"order":"04-04-marketplace-installation","title":"Install Kyma through the GCP Marketplace","source":"\nFollow these steps to quickly install Kyma through the GCP Marketpalce:\n\n1. Access **project Kyma** on the [Google Cloud Platform (GCP) Marketplace](https://console.cloud.google.com/marketplace/details/sap-public/kyma?q=kyma%20project) and click **CONFIGURE**.\n\n2. When the pop-up box appears, select the project in which you want to create a Kubernetes cluster and deploy Kyma.\n\n3. To create a Kubernetes cluster for your Kyma installation, select a cluster zone from the drop-down menu and click **Create cluster**. Wait for a few minutes for the Kubernetes cluster to provision.\n\n4. Adjust the basic settings of the Kyma deployment or use the default values:\n\n  | Field   |      Default value     |\n  |----------|-------------|\n  | **Namespace** | `default` |\n  | **App instance name** | `kyma-1` |\n  | **Cluster Admin Service Account** | `Create a new service account` |\n\n5. Accept the GCP Marketplace Terms of Service to continue.\n\n6. Click **Deploy** to install Kyma.\n\n>**NOTE:** The installation can take several minutes to complete.\n\n7. After you click **Deploy**, you're redirected to the **Applications** page under **Kubernetes Engine** in the GCP Console where you can check the installation status. When you see a green checkmark next to the application name, Kyma is installed. Follow the instructions from the **Next steps** section in **INFO PANEL** to add the Kyma self-signed TLS certificate to the trusted certificates of your OS.\n\n8. Access the cluster using the link and login details provided in the **Kyma info** section on the **Application details** page.\n\n>**TIP:** Watch [this](https://www.youtube.com/watch?v=hxVhQqI1B5A) video for a walkthrough of the installation process.","type":"Installation"},{"order":"04-05-use-your-own-domain","title":"Use your own domain","source":"\nThis guide explains how to deploy Kyma on a cluster using your own domain.\n\n>**TIP:** Get a free domain for your cluster using services like [freenom.com](https://www.freenom.com) or similar.\n\nChoose your cloud provider and get started:\n\n## Prerequisites\n\n<div tabs name=\"prerequisites\" group=\"use-your-own-domain\">\n  <details>\n  <summary label=\"gke\">\n  GKE\n  </summary>\n\n- A domain for your [Google Kubernetes Engine](https://cloud.google.com/kubernetes-engine/) (GKE) cluster\n- [Google Cloud Platform](https://console.cloud.google.com/) (GCP) project with Kubernetes Engine API enabled\n- [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) 1.14.6 or higher\n- [gcloud](https://cloud.google.com/sdk/gcloud/)\n- [wget](https://www.gnu.org/software/wget/)\n\n>**NOTE:** Running Kyma on GKE requires three [`n1-standard-4` machines](https://cloud.google.com/compute/docs/machine-types). You create these machines when you complete the **Prepare the GKE cluster** step.\n\n  </details>\n  <details>\n  <summary label=\"aks\">\n  AKS\n  </summary>\n\n- A domain for your [Azure Kubernetes Service](https://azure.microsoft.com/services/kubernetes-service/) (AKS) cluster\n- [Microsoft Azure](https://azure.microsoft.com)\n- [Kubernetes](https://kubernetes.io/) 1.14.6 or higher\n- Tiller 2.10.0 or higher\n- [Docker](https://www.docker.com/)\n- [Docker Hub](https://hub.docker.com/) account\n- [az](https://docs.microsoft.com/en-us/cli/azure/install-azure-cli)\n\n>**NOTE:** Running Kyma on AKS requires three [`Standard_D4_v3` machines](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-general#dv3-series-1). You create these machines when you complete the **Prepare the AKS cluster** step.\n\n>**CAUTION:** If you define your own Kubernetes jobs on the AKS cluster, follow [this](/docs/master/components/service-mesh/#troubleshooting-kubernetes-jobs-fail-on-aks) troubleshooting guide to avoid jobs running endlessly on AKS deployments of Kyma.\n\n  </details>\n\n</div>\n\n## Choose the release to install\n\n1. Go to [this](https://github.com/kyma-project/kyma/releases/) page and choose the release you want to install.\n\n2. Export the release version as an environment variable. Run:\n\n    ```bash\n    export KYMA_VERSION={KYMA_RELEASE_VERSION}\n    ```\n\n## DNS setup and TLS certificate generation\n\n<div tabs name=\"provider-installation\" group=\"use-your-own-domain\">\n  <details>\n  <summary label=\"gke\">\n  GKE\n  </summary>\n\n### Delegate the management of your domain to Google Cloud DNS\n\n> **NOTE**: Google Cloud DNS setup has to be done only once per a DNS zone.\n\nFollow these steps:\n\n1. Export the project name, domain name, and DNS zone name as environment variables. Run the following commands:\n\n    ```bash\n    export GCP_PROJECT={YOUR_GCP_PROJECT}\n    export DNS_NAME={YOUR_ZONE_DOMAIN}\n    export DNS_ZONE={YOUR_DNS_ZONE}\n    ```\n\n    Example:\n\n    ```bash\n    export DNS_NAME=kyma-demo.ga\n    export DNS_ZONE=myzone\n    ```\n\n2. Create a DNS-managed zone in your Google project. Run:\n\n    ```bash\n    gcloud dns --project=$GCP_PROJECT managed-zones create $DNS_ZONE --description= --dns-name=$DNS_NAME\n    ```\n\n    Alternatively, create the DNS-managed zone through the GCP UI. In the **Network** section navigate to **Network Services**, click **Cloud DNS** and select **Create Zone**.\n\n3. Delegate your domain to Google name servers.\n\n- Get the list of the name servers from the zone details. This is a sample list:\n\n   ```bash\n   ns-cloud-b1.googledomains.com.\n   ns-cloud-b2.googledomains.com.\n   ns-cloud-b3.googledomains.com.\n   ns-cloud-b4.googledomains.com.\n   ```\n\n- Set up your domain to use these name servers.\n\n4. Check if everything is set up correctly and your domain is managed by Google name servers. Run:\n\n    ```bash\n    host -t ns $DNS_NAME\n    ```\n\n    A successful response returns the list of the name servers you fetched from GCP.\n\n### Get the TLS certificate\n\n1. Export the certificate issuer email and the cluster domain as environment variables:\n\n    ```bash\n    export CERT_ISSUER_EMAIL={YOUR_EMAIL}\n    export DOMAIN=\"$CLUSTER_NAME.$(echo $DNS_NAME | sed `s/\\.$//`)\"\n    ```\n\n2. Create a folder for certificates. Run:\n\n    ```bash\n    mkdir letsencrypt\n    ```\n\n3. Create a new service account and assign it to the **dns.admin** role. Run these commands:\n\n    ```bash\n    gcloud iam service-accounts create dnsmanager --display-name \"dnsmanager\" --project \"$GCP_PROJECT\"\n    ```\n\n    ```bash\n    gcloud projects add-iam-policy-binding $GCP_PROJECT \\\n        --member serviceAccount:dnsmanager@$GCP_PROJECT.iam.gserviceaccount.com --role roles/dns.admin\n    ```\n\n    > **NOTE**: You don't have to create a new DNS manager service account (SA) every time you deploy a cluster. Instead, you can use an existing SA that has the **dns.admin** assigned.\n\n4. Generate an access key for this account in the `letsencrypt` folder. Run:\n\n    ```bash\n    gcloud iam service-accounts keys create ./letsencrypt/key.json --iam-account dnsmanager@$GCP_PROJECT.iam.gserviceaccount.com\n    ```\n\n    > **NOTE**: The number of keys you can generate for a single service account is limited. Reuse the existing keys instead of generating a new key for every cluster.\n\n5. Run the Certbot Docker image with the `letsencrypt` folder mounted. Certbot uses the key to apply DNS challenge for the certificate request and stores the TLS certificates in that folder. Run:\n\n    ```bash\n    docker run -it --name certbot --rm \\\n        -v \"$(pwd)/letsencrypt:/etc/letsencrypt\" \\\n        certbot/dns-google \\\n        certonly \\\n        -m $CERT_ISSUER_EMAIL --agree-tos --no-eff-email \\\n        --dns-google \\\n        --dns-google-credentials /etc/letsencrypt/key.json \\\n        --server https://acme-v02.api.letsencrypt.org/directory \\\n        -d \"*.$DOMAIN\"\n    ```\n\n6. Export the certificate and key as environment variables. Run these commands:\n\n    ```bash\n    export TLS_CERT=$(cat ./letsencrypt/live/$DOMAIN/fullchain.pem | base64 | sed 's/ /\\\\ /g' | tr -d '\\n');\n    export TLS_KEY=$(cat ./letsencrypt/live/$DOMAIN/privkey.pem | base64 | sed 's/ /\\\\ /g' | tr -d '\\n')\n    ```\n\n  </details>\n  <details>\n  <summary label=\"aks\">\n  AKS\n  </summary>\n\n### Delegate the management of your domain to Azure DNS\n\nFollow these steps:\n\n1. Export the domain name, the sub-domain, and the resource group name as environment variables. Run these commands:\n\n    ```bash\n    export DNS_DOMAIN={YOUR_DOMAIN} # example.com\n    export SUB_DOMAIN={YOUR_SUBDOMAIN} # cluster (in this case the full name of your cluster is cluster.example.com)\n    export DOMAIN=\"$SUB_DOMAIN.$DNS_DOMAIN\" # cluster.example.com\n    export RS_GROUP={YOUR_RESOURCE_GROUP_NAME}\n    ```\n\n2. Create a DNS-managed zone in your Azure subscription. Run:\n\n    ```bash\n    az network dns zone create -g $RS_GROUP -n $DNS_DOMAIN\n    ```\n\n    Alternatively, create it through the Azure UI. In the **Networking** section, go to **All services**, click **DNS zones**, and select **Add**.\n\n3. Delegate your domain to Azure name servers.\n\n- Get the list of the name servers from the zone details. This is a sample list:\n\n   ```bash\n   ns1-05.azure-dns.com.\n   ns2-05.azure-dns.net.\n   ns3-05.azure-dns.org.\n   ns4-05.azure-dns.info.\n   ```\n\n- Set up your domain to use these name servers.\n\n4. Check if everything is set up correctly and your domain is managed by Azure name servers. Run:\n\n    ```bash\n    host -t ns $DNS_DOMAIN\n    ```\n\nA successful response returns the list of the name servers you fetched from Azure.\n\n### Get the TLS certificate\n\n>**NOTE:** Azure DNS is not yet supported by Certbot so you must perform manual verification.\n\n1. Create a folder for certificates. Run:\n\n    ```bash\n    mkdir letsencrypt\n    ```\n\n2. Export your email address as an environment variable:\n\n    ```bash\n    export YOUR_EMAIL={YOUR_EMAIL}\n    ```\n\n3. To get the certificate, run the Certbot Docker image with the `letsencrypt` folder mounted. Certbot stores the TLS certificates in that folder.\n\n    ```bash\n    docker run -it --name certbot --rm \\\n        -v \"$(pwd)/letsencrypt:/etc/letsencrypt\" \\\n        certbot/certbot \\\n        certonly \\\n        -m $YOUR_EMAIL --agree-tos --no-eff-email \\\n        --manual \\\n        --manual-public-ip-logging-ok \\\n        --preferred-challenges dns \\\n        --server https://acme-v02.api.letsencrypt.org/directory \\\n        -d \"*.$SUB_DOMAIN.$DNS_DOMAIN\"\n    ```\n\n    You will see the following message:\n\n    ```bash\n    Please deploy a DNS TXT record under the name\n    _acme-challenge.rc2-test.kyma.online with the following value:\n\n    # TXT_VALUE\n\n    Before continuing, verify the record is deployed.\n\n    ```bash\n    Copy the `TXT_VALUE`.\n\n4. Open a new terminal and export these environment variables:\n\n    ```bash\n    export DNS_DOMAIN={YOUR_DOMAIN} # example.com\n    export SUB_DOMAIN={YOUR_SUBDOMAIN} # cluster (in this case the full name of your cluster is cluster.example.com)\n    export RS_GROUP={YOUR_RESOURCE_GROUP_NAME}\n    ```\n\n5. Export the `TXT_VALUE`.\n\n    ```bash\n    export TXT_VALUE={YOUR_TXT_VALUE}\n    ```\n\n    To modify the TXT record for your domain, run:\n\n    ```bash\n    az network dns record-set txt delete -n \"_acme-challenge.$SUB_DOMAIN\" -g $RS_GROUP -z $DNS_DOMAIN --yes\n    az network dns record-set txt create -n \"_acme-challenge.$SUB_DOMAIN\" -g $RS_GROUP -z $DNS_DOMAIN --ttl 60 > /dev/null\n    az network dns record-set txt add-record -n \"_acme-challenge.$SUB_DOMAIN\" -g $RS_GROUP -z $DNS_DOMAIN --value $TXT_VALUE\n    ```\n\n6. Go back to the first console, wait about 2 minutes and press enter.\n\n7. Export the certificate and key as environment variables. Run these commands:\n\n    ```bash\n    export TLS_CERT=$(cat ./letsencrypt/live/$SUB_DOMAIN.$DNS_DOMAIN/fullchain.pem | base64 | sed 's/ /\\\\ /g')\n    export TLS_KEY=$(cat ./letsencrypt/live/$SUB_DOMAIN.$DNS_DOMAIN/privkey.pem | base64 | sed 's/ /\\\\ /g')\n    ```\n\n  </details>\n\n</div>\n\n## Prepare the cluster\n\n<div tabs name=\"provider-installation\" group=\"use-your-own-domain\">\n  <details>\n  <summary label=\"gke\">\n  GKE\n  </summary>\n\n1. Select a name for your cluster. Export the cluster name and the [zone](https://cloud.google.com/compute/docs/regions-zones/) you want to deploy to as environment variables. Run:\n\n    ```bash\n    export CLUSTER_NAME={CLUSTER_NAME_YOU_WANT}\n    export GCP_ZONE={GCP_ZONE_TO_DEPLOY_TO}\n    ```\n\n2. Create a cluster in the zone defined in the previous step. Run:\n\n    ```bash\n    gcloud container --project \"$GCP_PROJECT\" clusters \\\n    create \"$CLUSTER_NAME\" --zone \"$GCP_ZONE\" \\\n    --cluster-version \"1.14\" --machine-type \"n1-standard-4\" \\\n    --addons HorizontalPodAutoscaling,HttpLoadBalancing\n    ```\n\n3. Configure kubectl to use your new cluster. Run:\n\n    ```bash\n    gcloud container clusters get-credentials $CLUSTER_NAME --zone $GCP_ZONE --project $GCP_PROJECT\n    ```\n\n4. Add your account as the cluster administrator:\n\n    ```bash\n    kubectl create clusterrolebinding cluster-admin-binding --clusterrole=cluster-admin --user=$(gcloud config get-value account)\n    ```\n\n5. Install custom installation overrides for your DNS domain and TLC certifcates. Run:\n\n    ```bash\n    kubectl create namespace kyma-installer \\\n    && kubectl create configmap owndomain-overrides -n kyma-installer --from-literal=global.domainName=$DOMAIN --from-literal=global.tlsCrt=$TLS_CERT --from-literal=global.tlsKey=$TLS_KEY \\\n    && kubectl label configmap owndomain-overrides -n kyma-installer installer=overrides\n    ```\n\n>**TIP:** An example config map is available [here](./assets/owndomain-overrides.yaml).\n\n</details>\n  <details>\n  <summary label=\"aks\">\n  AKS\n  </summary>\n\n1. Select a name for your cluster. Set the cluster name, the resource group and region as environment variables. Run:\n\n    ```bash\n    export RS_GROUP={YOUR_RESOURCE_GROUP_NAME}\n    export CLUSTER_NAME={YOUR_CLUSTER_NAME}\n    export REGION={YOUR_REGION} #westeurope\n    ```\n\n2. Create a resource group that will contain all your resources:\n\n    ```bash\n    az group create --name $RS_GROUP --location $REGION\n    ```\n\n3. Create an AKS cluster. Run:\n\n    ```bash\n    az aks create \\\n      --resource-group $RS_GROUP \\\n      --name $CLUSTER_NAME \\\n      --node-vm-size \"Standard_D4_v3\" \\\n      --kubernetes-version 1.14.6 \\\n      --enable-addons \"monitoring,http_application_routing\" \\\n      --generate-ssh-keys\n    ```\n\n4. To configure kubectl to use your new cluster, run:\n\n    ```bash\n    az aks get-credentials --resource-group $RS_GROUP --name $CLUSTER_NAME\n    ```\n\n5. Add additional privileges to be able to access readiness probes endpoints on your AKS cluster.\n\n    ```bash\n    kubectl apply -f https://raw.githubusercontent.com/kyma-project/kyma/$KYMA_RELEASE_VERSION/installation/resources/azure-crb-for-healthz.yaml\n    ```\n\n6. Install custom installation overrides for AKS, your DNS domain and TLC certifcates. Run:\n\n    ```bash\n    kubectl create namespace kyma-installer \\\n    && kubectl create configmap owndomain-overrides -n kyma-installer --from-literal=global.domainName=$DOMAIN --from-literal=global.tlsCrt=$TLS_CERT --from-literal=global.tlsKey=$TLS_KEY \\\n    && kubectl label configmap owndomain-overrides -n kyma-installer installer=overrides \\\n    && kubectl create configmap aks-overrides -n kyma-installer --from-literal=global.proxy.excludeIPRanges=10.0.0.1 \\\n    && kubectl label configmap aks-overrides -n kyma-installer installer=overrides component=istio\n    ```\n\n    >**TIP:** An example config maps are available [here](./assets/owndomain-overrides.yaml) and [here](./assets/aks-overrides.yaml).\n\n  </details>\n\n</div>\n\n## Install Kyma\n\n1. Install Tiller on the cluster you provisioned. Run:\n\n   ```bash\n   kubectl apply -f https://raw.githubusercontent.com/kyma-project/kyma/$KYMA_VERSION/installation/resources/tiller.yaml\n   ```\n\n2. Deploy Kyma. Run:\n\n    ```bash\n    kubectl apply -f https://github.com/kyma-project/kyma/releases/download/$KYMA_VERSION/kyma-installer-cluster.yaml\n    ```\n\n3. Check if the Pods of Tiller and the Kyma Installer are running:\n\n    ```bash\n    kubectl get pods --all-namespaces\n    ```\n\n4. To watch the installation progress, run:\n\n    ```bash\n    while true; do \\\n      kubectl -n default get installation/kyma-installation -o jsonpath=\"{'Status: '}{.status.state}{', description: '}{.status.description}\"; echo; \\\n      sleep 5; \\\n    done\n    ```\n\nAfter the installation process is finished, the `Status: Installed, description: Kyma installed` message appears.\n\nIf you receive an error, fetch the Kyma Installer logs using this command:\n\n```bash\nkubectl -n kyma-installer logs -l 'name=kyma-installer'\n```\n\n## Configure DNS for the cluster load balancer\n\n<div tabs name=\"provider-installation\" group=\"use-your-own-domain\">\n  <details>\n  <summary label=\"gke\">\n  GKE\n  </summary>\n\nTo add DNS entries, run these commands:\n\n```bash\nexport EXTERNAL_PUBLIC_IP=$(kubectl get service -n istio-system istio-ingressgateway -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\n\nexport APISERVER_PUBLIC_IP=$(kubectl get service -n kyma-system apiserver-proxy-ssl -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\n\ngcloud dns --project=$GCP_PROJECT record-sets transaction start --zone=$DNS_ZONE\n\ngcloud dns --project=$GCP_PROJECT record-sets transaction add $EXTERNAL_PUBLIC_IP --name=\\*.$DOMAIN. --ttl=60 --type=A --zone=$DNS_ZONE\n\ngcloud dns --project=$GCP_PROJECT record-sets transaction add $APISERVER_PUBLIC_IP --name=\\apiserver.$DOMAIN. --ttl=60 --type=A --zone=$DNS_ZONE\n\ngcloud dns --project=$GCP_PROJECT record-sets transaction execute --zone=$DNS_ZONE\n```\n\n  </details>\n  <details>\n  <summary label=\"aks\">\n  AKS\n  </summary>\n\nRun these commands:\n\n```bash\nexport EXTERNAL_PUBLIC_IP=$(kubectl get service -n istio-system istio-ingressgateway -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\n\nexport APISERVER_PUBLIC_IP=$(kubectl get service -n kyma-system apiserver-proxy-ssl -o jsonpath=\"{.status.loadBalancer.ingress[0].ip}\")\n\naz network dns record-set a create -g $RS_GROUP -z $DNS_DOMAIN -n \\*.$SUB_DOMAIN --ttl 60\naz network dns record-set a add-record -g $RS_GROUP -z $DNS_DOMAIN -n \\*.$SUB_DOMAIN -a $EXTERNAL_PUBLIC_IP\n\naz network dns record-set a create -g $RS_GROUP -z $DNS_DOMAIN -n apiserver.$SUB_DOMAIN --ttl 60\naz network dns record-set a add-record -g $RS_GROUP -z $DNS_DOMAIN -n apiserver.$SUB_DOMAIN -a $APISERVER_PUBLIC_IP\n```\n\n  </details>\n\n</div>\n\n### Access the cluster\n\n1. To get the address of the cluster's Console, check the host of the Console's virtual service. The name of the host of this virtual service corresponds to the Console URL. To get the virtual service host, run:\n\n    ```bash\n    kubectl get virtualservice core-console -n kyma-system -o jsonpath='{ .spec.hosts[0] }'\n    ```\n\n2. Access your cluster under this address:\n\n    ```bash\n    https://{VIRTUAL_SERVICE_HOST}\n    ```\n\n3. To log in to your cluster's Console UI, use the default `admin` static user. Click **Login with Email** and sign in with the **admin@kyma.cx** email address. Use the password contained in the `admin-user` Secret located in the `kyma-system` Namespace. To get the password, run:\n\n    ```bash\n    kubectl get secret admin-user -n kyma-system -o jsonpath=\"{.data.password}\" | base64 --decode\n    ```","type":"Installation"},{"order":"04-06-use-your-own-image","title":"Use your own Kyma Installer image","source":"\nWhen you install Kyma from a release, you use the release artifacts that already contain the Kyma Installer - a Docker image containing the combined binary of the Kyma Operator and the component charts from the `/resources` folder.\nIf you install Kyma from sources and use the latest `master` branch, you must build the image yourself to prepare the configuration file for Kyma installation on a GKE or AKS cluster. You also require a new image if you add components and custom Helm charts that are not included in the `/resources` folder to the installation.\n\nIn addition to the tools required to install Kyma on a cluster, you also need:\n- [Docker](https://www.docker.com/)\n- [Docker Hub](https://hub.docker.com/) account or any other Docker registry\n\n1. Clone the Kyma repository to your machine using either HTTPS or SSH. Run this command to clone the repository and change your working directory to `kyma`:\n    <div tabs>\n      <details>\n      <summary>\n      HTTPS\n      </summary>\n\n      ```\n      git clone https://github.com/kyma-project/kyma.git ; cd kyma\n      ```\n      </details>\n      <details>\n      <summary>\n      SSH\n      </summary>\n\n      ```\n      git clone git@github.com:kyma-project/kyma.git ; cd kyma\n      ```\n      </details>\n    </div>\n\n2. Build a Kyma-Installer image that is based on the current Kyma Operator binary and includes the current installation configurations and resources charts. Run:\n    ```\n    docker build -t kyma-installer -f tools/kyma-installer/kyma.Dockerfile .\n    ```\n\n3. Push the image to your Docker Hub. Run:\n    ```\n    docker tag kyma-installer:latest {YOUR_DOCKER_LOGIN}/kyma-installer\n    docker push {YOUR_DOCKER_LOGIN}/kyma-installer\n    ```\n\n4. Prepare the Kyma deployment file. Run this command:\n    ```\n    (cat installation/resources/installer.yaml ; echo \"---\" ; cat installation/resources/installer-cr-cluster.yaml.tpl) > my-kyma.yaml\n    ```\n\n5. The output of this operation is the `my-kyma.yaml` file.\nFind the following section in `my-kyma.yaml` and modify it to fetch the image you prepared. Change the `image` attribute value to `{YOUR_DOCKER_LOGIN}/kyma-installer`:\n    ```\n    spec:\n      template:\n        metadata:\n          labels:\n            name: kyma-installer\n        spec:\n          serviceAccountName: kyma-installer\n          containers:\n          - name: kyma-installer-container\n            image: {YOUR_DOCKER_LOGIN}/kyma-installer\n            imagePullPolicy: IfNotPresent\n    ```\n\n6. Use the `my-kyma.yaml` file to deploy Kyma. Choose the desired [installation option](#installation-overview) and run this command after you prepared the cluster:  \n    ```\n    kubectl apply -f my-kyma.yaml\n    ```","type":"Installation"},{"order":"04-08-custom-component-installation","title":"Custom component installation","source":"\nSince Kyma is modular, you can remove some components so that they are not installed together with Kyma. You can also add some of them after the installation process. Read this document to learn how to do that.\n\n## Remove a component\n\n>**NOTE:** Not all components can be simply removed from the Kyma installation. In case of Istio and the Service Catalog, you must provide your own deployment of these components in the Kyma-supported version before you remove them from the installation process. See [this](https://github.com/kyma-project/kyma/blob/master/resources/istio-kyma-patch/templates/job.yaml#L25) file to check the currently supported version of Istio. See [this](https://github.com/kyma-project/kyma/blob/master/resources/service-catalog/charts/catalog/values.yaml#L3) file to check the currently supported version of the Service Catalog.\n\nTo disable a component from the list of components that you install with Kyma, remove this component's entries from the appropriate file. The file differs depending on whether you install Kyma from the release or from sources, and if you install Kyma locally or on a cluster. The version of your component's deployment must match the version that Kyma currently supports.\n\n### Installation from the release\n\n1. Download the [newest version](https://github.com/kyma-project/kyma/releases) of Kyma.\n2. Customize installation by removing a component from the list of components in the Installation resource. For example, to disable the Application Connector installation, remove this entry:\n    ```\n    name: \"application-connector\"\n    namespace: \"kyma-system\"\n    ```\n  * from the `kyma-config-local.yaml` file for the **local** installation\n  * from the `kyma-config-cluster.yaml` file for the **cluster** installation\n\n\n3. Follow the installation steps described in the [Install Kyma locally from the release](#installation-install-kyma-locally-from-the-release) document, or [Install Kyma on a GKE cluster](#installation-install-kyma-on-a-gke-cluster) accordingly.\n\n### Installation from sources\n\n1. Customize installation by removing a component from the list of components in the following Installation resource:\n  * [installer-cr.yaml.tpl](https://github.com/kyma-project/kyma/blob/master/installation/resources/installer-cr.yaml.tpl) for the **local** installation\n  *  [installer-cr-cluster.yaml.tpl](https://github.com/kyma-project/kyma/blob/master/installation/resources/installer-cr-cluster.yaml.tpl) for the **cluster** installation\n\n2. Follow the installation steps described in the [Install Kyma locally from sources](#installation-install-kyma-locally-from-sources) document, or [Install Kyma on a GKE cluster](#installation-install-kyma-on-a-gke-cluster) accordingly.\n\n### Verify the installation\n\n1. Check if all Pods are running in the `kyma-system` Namespace:\n  ```\n  kubectl get pods -n kyma-system\n  ```\n2. Sign in to the Kyma Console using the `admin@kyma.cx` email address as described in the [Install Kyma locally from the release](#installation-install-kyma-locally-from-the-release) document.\n\n\n## Add a component\n\n>**NOTE:** This section assumes that you already have your Kyma Lite local version installed successfully.\n\nTo install a component that is not installed with Kyma by default, modify the [Installation](#custom-resource-installation) custom resource and add the component that you want to install to the list of components :\n\n1. Edit the resource:\n    ```\n    kubectl edit installation kyma-installation\n    ```\n2. Add the new component to the list of components, for example:\n    ```\n    - name: \"jaeger\"\n      namespace: \"kyma-system\"\n    ```\n3. Trigger the installation:\n   ```\n   kubectl label installation/kyma-installation action=install\n   ```\n\nYou can verify the installation status by calling `./installation/scripts/is-installed.sh` in the terminal.\n","type":"Installation"},{"order":"04-10-update","title":"Update Kyma","source":"\nThis guide describes how to update Kyma deployed locally or on a cluster.\n\n## Prerequisites\n\n- [Docker](https://www.docker.com/)\n- Access to a Docker Registry - only for cluster installation\n\n## Overview\n\nKyma consists of multiple components, installed as [Helm](https://github.com/helm/helm/tree/master/docs) releases.\n\nUpdate of an existing deployment can include:\n- changes in charts\n- changes in overrides\n- adding new releases\n\nThe update procedure consists of three main steps:\n- Prepare the update\n- Update the Kyma Installer\n- Trigger the update process\n\n> **NOTE:** In case of dependency conflicts or major changes between components versions, some updates may not be possible.\n\n> **NOTE:** Currently Kyma doesn't support removing components as a part of the update process.\n\n\n## Prepare the update\n\n- If you update an existing component, make all required changes to the Helm charts of the component located in the [`resource`](https://github.com/kyma-project/kyma/tree/master/resources) directory.\n\n- If you add a new component to your Kyma deployment, add a top-level Helm chart for that component. Additionally, run this command to edit the Installation custom resource and add the new component to the installed components list:\n  ```\n  kubectl edit installation kyma-installation\n  ```\n\n  > **NOTE:** Read [this](#custom-resource-installation) document to learn more about the Installation custom resource.\n\n\n- If you introduced changes in overrides, update the existing ConfigMaps and Secrets. Add new ConfigMaps and Secrets if required. See [this](#getting-started-helm-overrides-for-kyma-installation) document for more information on overrides.\n\n\n## Update the Kyma Installer on a local deployment\n\n- Build a new image for the Kyma Installer:  \n  ```\n  ./installation/scripts/build-kyma-installer.sh\n  ```  \n  > **NOTE:** If you started Kyma with the `run.sh` script with a `--vm-driver {value}` parameter, provide the same parameter to the `build-kyma-installer.sh` script.\n\n- Restart the Kyma Installer Pod:  \n  ```\n  kubectl delete pod -n kyma-installer {INSTALLER_POD_NAME}\n  ```\n\n## Update the Kyma Installer on a cluster deployment\n\n- Build a new image for the Kyma Installer:\n  ```\n  docker build -t {IMAGE_NAME}:{IMAGE_TAG} -f tools/kyma-installer/kyma.Dockerfile .\n  ```\n\n- Push the image to your Docker registry.\n\n- Redeploy the Kyma Installer Pod using the new image. Run this command to edit the Deployment configuration:\n  ```\n  kubectl edit deployment kyma-installer -n kyma-installer\n  ```\n  Change the `image` and `imagePullPolicy` attributes in this section:  \n    ```  \n         spec:\n           containers:\n           - image: <your_image_name>:<your_tag>\n             imagePullPolicy: Always\n    ```  \n  > **NOTE:** If the desired image name and `imagePullPolicy` is already set in the deployment configuration, restart the Pod by running `kubectl delete pod -n kyma-installer {INSTALLER_POD_NAME}`\n\n## Trigger the update process\n\nExecute the following command to trigger the update process:\n\n```\nkubectl label installation/kyma-installation action=install\n```\n","type":"Installation"},{"order":"04-11-local-reinstallation","title":"Reinstall Kyma","source":"\nThe custom scripts allow you to remove Kyma from a Minikube cluster and reinstall Kyma without removing the cluster.\n\n> **NOTE:** These scripts do not delete the cluster from your Minikube. This allows you to quickly reinstall Kyma.\n\n1. Use the `clean-up.sh` script to uninstall Kyma from the cluster. Run:\n  ```\n  scripts/clean-up.sh\n  ```\n\n2. Run this script to reinstall Kyma on an existing cluster:\n  ```\n  cmd/run.sh --skip-minikube-start\n  ```\n","type":"Installation"},{"order":"04-12-local-installation-scripts","title":"Local installation scripts deep-dive","source":"\nThis document extends the [Install Kyma locally from sources](#installation-install-kyma-locally-from-sources) guide with a detailed breakdown of the alternative local installation method which is the `run.sh` script.\n\nThe following snippet is the main element of the `run.sh` script:\n\n```\nif [[ ! $SKIP_MINIKUBE_START ]]; then\n    bash $SCRIPTS_DIR/minikube.sh --domain \"$DOMAIN\" --vm-driver \"$VM_DRIVER\" $MINIKUBE_EXTRA_ARGS\nfi\n\nbash $SCRIPTS_DIR/build-kyma-installer.sh --vm-driver \"$VM_DRIVER\"\n\nif [ -z \"$CR_PATH\" ]; then\n\n    TMPDIR=`mktemp -d \"$CURRENT_DIR/../../temp-XXXXXXXXXX\"`\n    CR_PATH=\"$TMPDIR/installer-cr-local.yaml\"\n    bash $SCRIPTS_DIR/create-cr.sh --output \"$CR_PATH\" --domain \"$DOMAIN\"\n\nfi\n\nbash $SCRIPTS_DIR/installer.sh --local --cr \"$CR_PATH\" --password \"$ADMIN_PASSWORD\"\nrm -rf $TMPDIR\n```\nSubsequent sections provide details of all involved subscripts, in the order in which the `run.sh` script triggers them.\n\n## The minikube.sh script\n\n> **NOTE:** To work with Kyma, use only the provided scripts and commands. Kyma does not work on a basic Minikube cluster that you can start using the `minikube start` command.\n\nThe purpose of the `installation/scripts/minikube.sh` script is to configure and start Minikube. The script also checks if your development environment is configured to handle the Kyma installation. This includes checking Minikube and kubectl versions.\n\nIf Minikube is already initialized, the system prompts you to agree to remove the previous Minikube cluster.\n- If you plan to perform a clean installation, answer `yes`.\n- If you installed Kyma to your Minikube cluster and then stopped the cluster using the `minikube stop` command, answer `no`.  This allows you to start the cluster again without reinstalling Kyma.\n\nMinikube is configured to disable the default Nginx Ingress Controller.\n\n>**NOTE:** For the complete list of parameters passed to the `minikube start` command, refer to the `installation/scripts/minikube.sh` script.\n\nOnce Minikube is up and running, the script adds local installation entries to `/etc/hosts`.\n\n## The build-kyma-installer.sh script\n\nThe Kyma Installer is an application based on the [Kubernetes operator](https://coreos.com/operators/). Its purpose is to install Helm charts defined in the Installation custom resource. The Kyma Installer is a Docker image that bundles the Installer binary with Kyma charts.\n\nThe `installation/scripts/build-kyma-installer.sh` script extracts the Kyma-Installer image name from the `installer.yaml` deployment file and uses it to build a Docker image inside Minikube. This image contains local Kyma sources from the `resources` folder.\n\n>**NOTE:** For the Kyma Installer Docker image details, refer to the `tools/kyma-installer/kyma.Dockerfile` file.\n\n## The create-cr.sh script\n\nThe `installation/scripts/create-cr.sh` script prepares the Installation custom resource from the `installation/resources/installer-cr.yaml.tpl` template. The local installation scenario uses the default Installation custom resource. The Kyma Installer already contains local Kyma resources bundled, thus `url` is ignored by the Installer component.\n\n>**NOTE:** Read [this](#custom-resource-installation) document to learn more about the Installation custom resource.\n\n## The installer.sh script\n\nThe `installation/scripts/installer.sh` script creates the default RBAC role, installs [Tiller](https://docs.helm.sh/), and deploys the Kyma Installer component.\n\n>**NOTE:** For the Kyma Installer deployment details, refer to the `installation/resources/installer.yaml` file.\n\nThe script applies the Installation custom resource and marks it with the `action=install` label, which triggers the Kyma installation.\n\nIn the process of installing Tiller, a set of TLS certificates is created and saved to [Helm Home](https://helm.sh/docs/glossary/#helm-home-helm-home) to secure the connection between the client and the server.\n\n>**NOTE:** The Kyma installation runs in the background. Execute the `./installation/scripts/is-installed.sh` script to follow the installation process.\n\n## The is-installed.sh script\n\nThe `installation/scripts/is-installed.sh` script shows the status of Kyma installation in real time. The script checks the status of the Installation custom resource. When it detects that the status changed to `Installed`, the script exits. If you define a timeout period and the status doesn't change to `Installed` within that period, the script fetches the Installer logs. If you don't set a timeout period, the script waits for the change of the status until you terminate it.\n","type":"Installation"},{"order":"06-01-installation","title":"Installation","source":"\nThe `installations.installer.kyma-project.io` CustomResourceDefinition (CRD) is a detailed description of the kind of data and the format used to control the Kyma Installer, a proprietary solution based on the\n[Kubernetes operator](https://coreos.com/operators/) principles. To get the up-to-date CRD and show the output in the `yaml` format, run this command:  \n\n```\nkubectl get crd installations.installer.kyma-project.io -o yaml\n```\n\n## Sample custom resource\n\nThis is a sample CR that controls the Kyma Installer. This example has the **action** label set to `install`, which means that it triggers the installation of Kyma. The  **name** and **namespace**  fields in the `components` array define which components you install and Namespaces in which you install them.\n\n>**NOTE:** See the `installer-cr.yaml.tpl` file in the `/installation/resources` directory for the complete list of Kyma components.\n\n```\napiVersion: \"installer.kyma-project.io/v1alpha1\"\nkind: Installation\nmetadata:\n  name: kyma-installation\n  labels:\n    action: install\n  finalizers:\n    - finalizer.installer.kyma-project.io\nspec:\n  version: \"1.0.0\"\n  url: \"https://sample.url.com/kyma_release.tar.gz\"\n  components:\n    - name: \"cluster-essentials\"\n      namespace: \"kyma-system\"\n    - name: \"istio\"\n      namespace: \"istio-system\"\n    - name: \"prometheus-operator\"\n      namespace: \"kyma-system\"\n    - name: \"provision-bundles\"\n    - name: \"dex\"\n      namespace: \"kyma-system\"\n    - name: \"core\"\n      namespace: \"kyma-system\"\n```\n\n## Custom resource parameters\n\nThis table lists all the possible parameters of a given resource together with their descriptions:\n\n| Field   |      Mandatory      |  Description |\n|----------|:-------------:|:------|\n| **metadata.name** | **YES** | Specifies the name of the CR. |\n| **metadata.labels.action** | **YES** | Defines the behavior of the Kyma Installer. Available options are `install` and `uninstall`. |\n| **metadata.finalizers** | **NO** | Protects the CR from deletion. Read [this](https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#finalizers) Kubernetes document to learn more about finalizers. |\n| **spec.version** | **NO** | When manually installing Kyma on a cluster, specify any valid [SemVer](https://semver.org/) notation string. |\n| **spec.url** | **YES** | Specifies the location of the Kyma sources `tar.gz` package. For example, for the `master` branch of Kyma, the address is `https://github.com/kyma-project/kyma/archive/master.tar.gz` |\n| **spec.components** | **YES** | Lists which components of Helm chart components to install or update. |\n| **spec.components.name** | **YES** | Specifies the name of the component which is the same as the name of the component subdirectory in the `resources` directory. |\n| **spec.components.namespace** | **YES** | Defines the Namespace in which you want the Installer to install, or update the component. |\n| **spec.components.release** | **NO** | Provides the name of the Helm release. The default parameter is the component name. |\n\n## Additional information\n\nThe Kyma Installer adds the **status** section which describes the status of Kyma installation. This table lists the fields of the **status** section.\n\n| Field   |      Mandatory      |  Description |\n|----------|:-------------:|:------|\n| **status.state** | **YES** | Describes the installation state. Takes one of four values. |\n| **status.description** | **YES** | Describes the installation step the installer performs at the moment. |\n| **status.errorLog** | **YES** | Lists all errors that happen during the installation. |\n| **status.errorLog.component** | **YES** | Specifies the name of the component that causes the error. |\n| **status.errorLog.log** | **YES** | Provides a description of the error. |\n| **status.errorLog.occurrences** | **YES** | Specifies the number of subsequent occurrences of the error. |\n\nThe **status.state** field uses one of the following four values to describe the installation state:\n\n|   State   |  Description |\n|----------|:-------------|\n| **Installed** | Installation successful. |\n| **Uninstalled** | Uninstallation successful. |\n| **InProgress** | The Installer is still installing or uninstalling Kyma. No errors logged. |\n| **Error** | The Installer encountered a problem but it continues to try to process the resource. |\n\n## Related resources and components\n\nThese components use this CR:\n\n| Component   |   Description |\n|----------|:------|\n| Installer  |  The CR triggers the Installer to install, update or delete of the specified components. |\n","type":"Custom Resource"},{"order":"08-01-sample-service-deployment-to-local","title":"Sample service deployment on local","source":"\nThis tutorial is intended for the developers who want to quickly learn how to deploy a sample service and test it with Kyma installed locally on Mac.\n\nThis tutorial uses a standalone sample service written in the [Go](http://golang.org) language .\n\n## Prerequisites\n\nTo use the Kyma cluster and install the example, download these tools:\n\n- [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) 1.10.0\n- [curl](https://github.com/curl/curl)\n\n## Steps\n\n### Deploy and expose a sample standalone service\n\nFollow these steps:\n\n1. Deploy the sample service to any of your Namespaces. Use the `stage` Namespace for this guide:\n\n   ```bash\n   kubectl create -n stage -f https://raw.githubusercontent.com/kyma-project/examples/master/http-db-service/deployment/deployment.yaml\n   ```\n\n2. Create an unsecured API for your example service:\n\n   ```bash\n   kubectl apply -n stage -f https://raw.githubusercontent.com/kyma-project/examples/master/gateway/service/api-without-auth.yaml\n   ```\n\n3. Add the IP address of Minikube to the `hosts` file on your local machine for your APIs:\n\n   ```bash\n   $ echo \"$(minikube ip) http-db-service.kyma.local\" | sudo tee -a /etc/hosts\n   ```\n\n4. Access the service using the following call:\n   ```bash\n   curl -ik https://http-db-service.kyma.local/orders\n   ```\n\n   The system returns a response similar to the following:\n   ```\n   HTTP/2 200\n   content-type: application/json;charset=UTF-8\n   vary: Origin\n   date: Mon, 01 Jun 2018 00:00:00 GMT\n   content-length: 2\n   x-envoy-upstream-service-time: 131\n   server: envoy\n\n   []\n   ```\n\n### Update your service's API to secure it\n\nRun the following command:\n\n   ```bash\n   kubectl apply -n stage -f https://raw.githubusercontent.com/kyma-project/examples/master/gateway/service/api-with-auth.yaml\n   ```\nAfter you apply this update, you must include a valid bearer ID token in the Authorization header to access the service.\n\n>**NOTE:** The update might take some time.\n","type":"Tutorials"},{"order":"08-02-sample-service-deployment-to-cluster","title":"Sample service deployment on a cluster","source":"\nThis tutorial is intended for the developers who want to quickly learn how to deploy a sample service and test it with the Kyma cluster.\n\nThis tutorial uses a standalone sample service written in the [Go](http://golang.org) language.\n\n## Prerequisites\n\nTo use the Kyma cluster and install the example, download these tools:\n\n- [kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/) 1.10.0\n- [curl](https://github.com/curl/curl)\n\n## Steps\n\n### Get the kubeconfig file and configure the CLI\n\nFollow these steps to get the `kubeconfig` file and configure the CLI to connect to the cluster:\n\n1. Access the Console UI of your Kyma cluster.\n2. Click **Administration**.\n3. Click the **Download config** button to download the `kubeconfig` file to a selected location on your machine.\n4. Open a terminal window.\n5. Export the **KUBECONFIG** environment variable to point to the downloaded `kubeconfig`. Run this command:\n  ```\n  export KUBECONFIG={KUBECONFIG_FILE_PATH}\n  ```\n  >**NOTE:** Drag and drop the `kubeconfig` file in the terminal to easily add the path of the file to the `export KUBECONFIG` command you run.\n\n6. Run `kubectl cluster-info` to check if the CLI is connected to the correct cluster.\n\n### Set the cluster domain as an environment variable\n\nThe commands in this guide use URLs in which you must provide the domain of the cluster that you use.\nExport the domain of your cluster as an environment variable. Run:  \n  ```\n  export yourClusterDomain='{YOUR_CLUSTER_DOMAIN}'\n  ```\n\n### Deploy and expose a sample standalone service\n\nFollow these steps:\n\n1. Deploy the sample service to any of your Namespaces. Use the `stage` Namespace for this guide:\n\n   ```bash\n   kubectl create -n stage -f https://raw.githubusercontent.com/kyma-project/examples/master/http-db-service/deployment/deployment.yaml\n   ```\n\n2. Create an unsecured API for your service:\n\n   ```bash\n   curl -k https://raw.githubusercontent.com/kyma-project/examples/master/gateway/service/api-without-auth.yaml |  sed \"s/.kyma.local/.$yourClusterDomain/\" | kubectl apply -n stage -f -\n   ```\n\n3. Access the service using the following call:\n   ```bash\n   curl -ik https://http-db-service.$yourClusterDomain/orders\n   ```\n\n   The system returns a response similar to the following:\n   ```\n   HTTP/2 200\n   content-type: application/json;charset=UTF-8\n   vary: Origin\n   date: Mon, 01 Jun 2018 00:00:00 GMT\n   content-length: 2\n   x-envoy-upstream-service-time: 131\n   server: envoy\n\n   []\n   ```\n\n### Update your service's API to secure it\n\nRun the following command:\n\n   ```bash\n   curl -k https://raw.githubusercontent.com/kyma-project/examples/master/gateway/service/api-with-auth.yaml |  sed \"s/.kyma.local/.$yourClusterDomain/\" | kubectl apply -n stage -f -\n   ```\nAfter you apply this update, you must include a valid bearer ID token in the Authorization header to access the service.\n\n>**NOTE:** The update might take some time.\n","type":"Tutorials"},{"order":"08-03-local-develop-no-docker","title":"Develop a service locally without using Docker","source":"\nYou can develop services in the local Kyma installation without extensive Docker knowledge or a need to build and publish a Docker image. The `minikube mount` feature allows you to mount a directory from your local disk into the local Kubernetes cluster.\n\nThis tutorial shows how to use this feature, using the service example implemented in Golang.\n\n## Prerequisites\n\nInstall [Golang](https://golang.org/dl/).\n\n## Steps\n\n### Install the example on your local machine\n\n1. Install the example:\n```shell\ngo get -insecure github.com/kyma-project/examples/http-db-service\n```\n2. Navigate to installed example and the `http-db-service` folder inside it:\n```shell\ncd ~/go/src/github.com/kyma-project/examples/http-db-service\n```\n3. Build the executable to run the application:\n```shell\nCGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main .\n```\n\n### Mount the example directory into Minikube\n\nFor this step, you need a running local Kyma instance. Read [this](#installation-install-kyma-locally-from-the-release) document to learn how to install Kyma locally.\n\n1. Open the terminal window. Do not close it until the development finishes.\n2. Mount your local drive into Minikube:\n```shell\n# Use the following pattern:\nminikube mount {LOCAL_DIR_PATH}:{CLUSTER_DIR_PATH}`\n# To follow this guide, call:\nminikube mount ~/go/src/github.com/kyma-project/examples/http-db-service:/go/src/github.com/kyma-project/examples/http-db-service\n```\n\nSee the example and expected result:\n```shell\n# Terminal 1\n$ minikube mount ~/go/src/github.com/kyma-project/examples/http-db-service:/go/src/github.com/kyma-project/examples/http-db-service\n\nMounting /Users/{USERNAME}/go/src/github.com/kyma-project/examples/http-db-service into /go/src/github.com/kyma-project/examples/http-db-service on the minikube VM\nThis daemon process must stay alive for the mount to still be accessible...\nufs starting\n```\n\n### Run your local service inside Minikube\n\n1. Create Pod that uses the base Golang image to run your executable located on your local machine:\n```shell\n# Terminal 2\nkubectl run mydevpod --image=golang:1.9.2-alpine --restart=Never -n stage --overrides='\n{\n   \"spec\":{\n      \"containers\":[\n         {\n            \"name\":\"mydevpod\",\n            \"image\":\"golang:1.9.2-alpine\",\n            \"command\": [\"./main\"],\n            \"workingDir\":\"/go/src/github.com/kyma-project/examples/http-db-service\",\n            \"volumeMounts\":[\n               {\n                  \"mountPath\":\"/go/src/github.com/kyma-project/examples/http-db-service\",\n                  \"name\":\"local-disk-mount\"\n               }\n            ]\n         }\n      ],\n      \"volumes\":[\n         {\n            \"name\":\"local-disk-mount\",\n            \"hostPath\":{\n               \"path\":\"/go/src/github.com/kyma-project/examples/http-db-service\"\n            }\n         }\n      ]\n   }\n}\n'\n```\n2. Expose the Pod as a service from Minikube to verify it:\n```shell\nkubectl expose pod mydevpod --name=mypodservice --port=8017 --type=NodePort -n stage\n```\n3. Check the Minikube IP address and Port, and use them to access your service.\n```shell\n# Get the IP address.\nminikube ip\n# See the example result: 192.168.64.44\n# Check the Port.\nkubectl get services -n stage\n# See the example result: mypodservice  NodePort 10.104.164.115  <none>  8017:32226/TCP  5m\n```\n4. Call the service from your terminal.\n```shell\ncurl {minikube ip}:{port}/orders -v\n# See the example: curl http://192.168.64.44:32226/orders -v\n# The command returns an empty array.\n```\n\n### Modify the code locally and see the results immediately in Minikube\n\n1. Edit the `main.go` file by adding a new `test` endpoint to the `startService` function\n```go\nrouter.HandleFunc(\"/test\", func (w http.ResponseWriter, r *http.Request) {\n\tw.Write([]byte(\"test\"))\n})\n```\n2. Build a new executable to run the application inside Minikube:\n```shell\nCGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o main .\n```\n3. Replace the existing Pod with the new version:\n```shell\nkubectl get pod mydevpod -n stage -o yaml | kubectl replace --force -f -\n```\n4. Call the new `test` endpoint of the service from your terminal. The command returns the `Test` string:\n```shell\ncurl http://192.168.64.44:32226/test -v\n```\n","type":"Tutorials"},{"order":"08-04-publish-service-image-and-deploy","title":"Publish a service Docker image and deploy it to Kyma","source":"\nFollow this tutorial to learn how to develop a service locally. You can immediately see all the changes made in a local Kyma installation based on Minikube, without building a Docker image and publishing it to a Docker registry, such as the Docker Hub.\n\nUsing the same example service, this tutorial explains how to build a Docker image for your service, publish it to the Docker registry, and deploy it to the local Kyma installation. The instructions base on Minikube, but you can also use the image that you create and the Kubernetes resource definitions that you use on the Kyma cluster.\n\n>**NOTE:** The deployment works both on local Kyma installation and on the Kyma cluster.\n\n## Steps\n\n### Build a Docker image\n\nThe `http-db-service` example used in this guide provides you with the `Dockerfile` necessary for building Docker images. Examine the `Dockerfile` to learn how it looks and how it uses the Docker Multistaging feature, but do not use it one-to-one for production. There might be custom `LABEL` attributes with values to override.\n\n1. In your terminal, go to the `examples/http-db-service` directory. If you did not follow the [Sample service deployment on local](#tutorials-sample-service-deployment-on-local) guide and you do not have this directory locally, get the `http-db-service` example from the [`examples`](https://github.com/kyma-project/examples) repository.\n2. Run the build with `./build.sh`.\n\n>**NOTE:** Ensure that the new image builds and is available in your local Docker registry by calling `docker images`. Find an image called `example-http-db-service` and tagged as `latest`.\n\n### Register the image in the Docker Hub\n\nThis guide bases on Docker Hub. However, there are many other Docker registries available. You can use a private Docker registry, but it must be available in the Internet. For more details about using a private Docker registry, see the [this](#tutorials-publish-a-service-docker-image-and-deploy-it-to-kyma) document.\n\n1. Open the [Docker Hub](https://hub.docker.com/) webpage.\n2. Provide all of the required details and sign up.\n\n### Sign in to the Docker Hub registry in the terminal\n\n1. Call `docker login`.\n2. Provide the username and password, and select the `ENTER` key.\n\n### Push the image to the Docker Hub\n\n1. Tag the local image with a proper name required in the registry: `docker tag example-http-db-service {USERNAME}/example-http-db-service:0.0.1`.\n2. Push the image to the registry: `docker push {USERNAME}/example-http-db-service:0.0.1`.\n\n>**NOTE:** To verify if the image is successfully published, check if it is available online at the following address: `https://hub.docker.com/r/{USERNAME}/example-http-db-service/`\n\n### Deploy to Kyma\n\nThe `http-db-service` example contains sample Kubernetes resource definitions needed for the basic Kyma deployment. Find them in the `deployment` folder. Perform the following modifications to use your newly-published image in the local Kyma installation:\n\n1. Go to the `deployment` directory.\n2. Edit the `deployment.yaml` file. Change the **image** attribute to `{USERNAME}/example-http-db-service:0.0.1`.\n3. Create the new resources in local Kyma using these commands: `kubectl create -f deployment.yaml -n stage && kubectl create -f ingress.yaml -n stage`.\n4. Edit your `/etc/hosts` to add the new `http-db-service.kyma.local` host to the list of hosts associated with your `minikube ip`. Follow these steps:\n    - Open a terminal window and run: `sudo vim /etc/hosts`\n    - Select the **i** key to insert a new line at the top of the file.\n    - Add this line: `{YOUR.MINIKUBE.IP} http-db-service.kyma.local`\n    - Type `:wq` and select the **Enter** key to save the changes.\n5. Run this command to check if you can access the service: `curl https://http-db-service.kyma.local/orders`. The response should return an empty array.\n","type":"Tutorials"},{"order":"08-05-installation-overrides","title":"Helm overrides for Kyma installation","source":"\nKyma packages its components into [Helm](https://github.com/helm/helm/tree/master/docs) charts that the [Installer](https://github.com/kyma-project/kyma/tree/master/components/installer) uses.\nThis document describes how to configure the Installer with override values for Helm [charts](https://github.com/helm/helm/blob/master/docs/charts.md).\n\n\n## Overview\n\nThe Installer is a Kubernetes Operator that uses Helm to install Kyma components.\nHelm provides an overrides feature to customize the installation of charts, such as to configure environment-specific values.\nWhen using Installer for Kyma installation, users can't interact with Helm directly. The installation is not an interactive process.\n\nTo customize the Kyma installation, the Installer exposes a generic mechanism to configure Helm overrides called **user-defined** overrides.\n\n\n## User-defined overrides\n\nThe Installer finds user-defined overrides by reading the ConfigMaps and Secrets deployed in the `kyma-installer` Namespace and marked with the `installer:overrides` label.\n\nThe Installer constructs a single override by inspecting the ConfigMap or Secret entry key name. The key name should be a dot-separated sequence of strings corresponding to the structure of keys in the chart's `values.yaml` file or the entry in chart's template. See the examples below.\n\nInstaller merges all overrides recursively into a single YAML stream and passes it to Helm during the Kyma installation/upgrade operation.\n\n\n## Common vs component overrides\n\nThe Installer looks for available overrides each time a component installation or update operation is due.\nOverrides for the component are composed from two sets: **common** overrides and **component-specific** overrides.\n\nKyma uses common overrides for the installation of all components. ConfigMaps and Secrets marked with the label `installer:overrides`, contain the definition. They require no additional label.\n\nKyma uses component-specific overrides only for the installation of specific components. ConfigMaps and Secrets marked with both `installer:overrides` and `component: <name>` labels, where `<name>` is the component name, contain the definition. Component-specific overrides have precedence over Common ones in case of conflicting entries.\n\n\n## Overrides Examples\n\n### Top-level charts overrides\n\nOverrides for top-level charts are straightforward. Just use the template value from the chart (without leading \".Values.\" prefix) as the entry key in the ConfigMap or Secret.\n\nExample:\n\nThe Installer uses a `core` top-level chart that contains a template with the following value reference:\n```\nmemory: {{ .Values.test.acceptance.ui.requests.memory }}\n```\nThe chart's default value `test.acceptance.ui.requests.memory` in the `values.yaml` file resolves the template.\nThe following fragment of `values.yaml` shows this definition:\n```\ntest:\n  acceptance:\n    ui:\n      requests:\n        memory: \"1Gi\"\n```\n\nTo override this value, for example to \"2Gi\", proceed as follows:\n- Create a ConfigMap in the `kyma-installer` Namespace, labelled with: `installer:overrides` (or reuse an existing one).\n- Add an entry `test.acceptance.ui.requests.memory: 2Gi` to the map.\n\nOnce the installation starts, the Installer generates overrides based on the map entries. The system uses the value of \"2Gi\" instead of the default \"1Gi\" from the chart `values.yaml` file.\n\nFor overrides that the system should keep in Secrets, just define a Secret object instead of a ConfigMap with the same key and a base64-encoded value. Be sure to label the Secret with `installer:overrides`.\n\n\n### Sub-chart overrides\n\nOverrides for sub-charts follow the same convention as top-level charts. However, overrides require additional information about sub-chart location.\n\nWhen a sub-chart contains the `values.yaml` file, the information about the chart location is not necessary because the chart and it's `values.yaml` file are on the same level in the directory hierarchy.\n\nThe situation is different when the Installer installs a chart with sub-charts.\nAll template values for a sub-chart must be prefixed with a sub-chart \"path\" that is relative to the top-level \"parent\" chart.\n\nThis is not an Installer-specific requirement. The same considerations apply when you provide overrides manually using the `helm` command-line tool.\n\nHere is an example.\nThere's a `core` top-level chart, that the Installer installs.\nThere's an `application-connector` sub-chart in `core` with another nested sub-chart: `connector-service`.\nIn one of its templates there's a following fragment (shortened):\n\n```\nspec:\n  containers:\n  - name: {{ .Chart.Name }}\n\targs:\n\t  - \"/connectorservice\"\n\t  - '--appName={{ .Chart.Name }}'\n\t  - \"--domainName={{ .Values.global.domainName }}\"\n\t  - \"--tokenExpirationMinutes={{ .Values.deployment.args.tokenExpirationMinutes }}\"\n```\n\nThe following fragment of the `values.yaml` file in `connector-service` chart defines the default value for `tokenExpirationMinutes`:\n\n```\ndeployment:\n  args:\n    tokenExpirationMinutes: 60\n```\n\nTo override this value, such as to change \"60 to \"90\", do the following:\n\n- Create a ConfigMap in the `kyma-installer` Namespace labeled with `installer:overrides` or reuse existing one.\n- Add an entry `application-connector.connector-service.deployment.args.tokenExpirationMinutes: 90` to the map.\n\nNotice that the user-provided override key now contains two parts:\n\n- The chart \"path\" inside the top-level `core` chart: `application-connector.connector-service`\n- The original template value reference from the chart without the .Values. prefix: `deployment.args.tokenExpirationMinutes`.\n\nOnce the installation starts, the Installer generates overrides based on the map entries. The system uses the value of \"90\" instead of the default value of \"60\" from the `values.yaml` chart file.\n\n\n## Global overrides\n\nThere are several important parameters usually shared across the charts.\nHelm convention to provide these requires the use of the `global` override key.\nFor example, to define the `global.domain` override, just use \"global.domain\" as the name of the key in ConfigMap or Secret for the Installer.\n\nOnce the installation starts, the Installer merges all of the map entries and collects all of the global entries under the `global` top-level key to use for installation.\n\n\n## Values and types\n\nInstaller generally recognizes all override values as strings. It internally renders overrides to Helm as a YAML stream with only string values.\n\nThere is one exception to this rule with respect to handling booleans:\nThe system converts \"true\" or \"false\" strings that it encounters to a corresponding boolean value (true/false).\n\n\n## Merging and conflicting entries\n\nWhen the Installer encounters two overrides with the same key prefix, it tries to merge them.\nIf both of them represent a map (they have nested sub-keys), their nested keys are recursively merged.\nIf at least one of keys points to a final value, the Installer performs the merge in a non-deterministic order, so either one of the overrides is rendered in the final YAML data.\n\nIt is important to avoid overrides having the same keys for final values.\n\n\n### Example of non-conflicting merge:\n\nTwo overrides with a common key prefix (\"a.b\"):\n\n```\n\"a.b.c\": \"first\"\n\"a.b.d\": \"second\"\n```\n\nThe Installer yields correct output:\n\n```\na:\n  b:\n    c: first\n    d: second\n```\n\n### Example of conflicting merge:\n\nTwo overrides with the same key (\"a.b\"):\n\n```\n\"a.b\": \"first\"\n\"a.b\": \"second\"\n```\n\nThe Installer yields either:\n\n```\na:\n  b: \"first\"\n```\n\nOr (due to non-deterministic merge order):\n\n```\na:\n  b: \"second\"\n```\n","type":"Tutorials"},{"order":"12-01-try-out-kyma","title":"Kyma features and concepts in practice","source":"\nThe table contains a list of examples that demonstrate Kyma functionalities. You can run all of them locally or on a cluster. Examples are organized by a feature or concept they showcase. Each of them contains ready-to-use code snippets and the instructions in `README.md` documents.\n\nFollow the links to examples' code and content sources, and try them on your own.\n\n| Example | Description | Technology |\n|---|---|---|\n| [HTTP DB Service](https://github.com/kyma-project/examples/blob/master/http-db-service/README.md) | Test the service that exposes an HTTP API to access a database on the cluster. | Go, MSSQL |\n| [Event Service Subscription](https://github.com/kyma-project/examples/blob/master/event-subscription/service/README.md) | Test the example that demonstrates the `publish` and `consume` features of the Event Bus. | Go |\n| [Event Lambda Subscription](https://github.com/kyma-project/examples/blob/master/event-subscription/lambda/README.md) | Create functions, trigger them on Events, and bind them to services.  | Kubeless |\n| [Gateway](https://github.com/kyma-project/examples/blob/master/gateway/README.md) | Expose APIs for functions or services.  | Kubeless |\n| [Service Binding](https://github.com/kyma-project/examples/blob/master/service-binding/lambda/README.md) | Bind a Redis service to a lambda function. | Kubeless, Redis, NodeJS |\n| [Call SAP Commerce](https://github.com/kyma-project/examples/blob/master/call-ec/README.md) | Call SAP Commerce in the context of the end user. | Kubeless, NodeJS |\n| [Alert Rules](https://github.com/kyma-project/examples/blob/master/monitoring-alert-rules/README.md) | Configure alert rules in Kyma.  | Prometheus |\n| [Custom Metrics in Kyma](https://github.com/kyma-project/examples/blob/master/monitoring-custom-metrics/README.md) | Expose custom metrics in Kyma.  | Go, Prometheus |\n| [Event Email Service](https://github.com/kyma-project/examples/blob/master/event-email-service/README.md) | Send an automated email upon receiving an Event.  | NodeJS |\n| [Tracing](https://github.com/kyma-project/examples/blob/master/tracing/README.md) | Configure tracing for a service in Kyma. | Go |\n","type":"Examples"}]},"navigation":{"root":[{"displayName":"Kyma","id":"kyma"}],"components":[{"displayName":"Security","id":"security"},{"displayName":"Service Catalog","id":"service-catalog"},{"displayName":"Helm Broker","id":"helm-broker"},{"displayName":"Application Connector","id":"application-connector"},{"displayName":"Event Bus","id":"event-bus"},{"displayName":"Service Mesh","id":"service-mesh"},{"displayName":"Serverless","id":"serverless"},{"displayName":"Monitoring","id":"monitoring"},{"displayName":"Tracing","id":"tracing"},{"displayName":"API Gateway","id":"api-gateway"},{"displayName":"Logging","id":"logging"},{"displayName":"Backup","id":"backup"},{"displayName":"Console","id":"console"},{"displayName":"Asset Store","id":"asset-store"},{"displayName":"Headless CMS","id":"headless-cms"}]},"manifest":{"root":[{"displayName":"Kyma","id":"kyma"}],"components":[{"displayName":"Security","id":"security"},{"displayName":"Service Catalog","id":"service-catalog"},{"displayName":"Helm Broker","id":"helm-broker"},{"displayName":"Application Connector","id":"application-connector"},{"displayName":"Event Bus","id":"event-bus"},{"displayName":"Service Mesh","id":"service-mesh"},{"displayName":"Serverless","id":"serverless"},{"displayName":"Monitoring","id":"monitoring"},{"displayName":"Tracing","id":"tracing"},{"displayName":"API Gateway","id":"api-gateway"},{"displayName":"Logging","id":"logging"},{"displayName":"Backup","id":"backup"},{"displayName":"Console","id":"console"},{"displayName":"Asset Store","id":"asset-store"},{"displayName":"Headless CMS","id":"headless-cms"}]},"assetsPath":"/assets/docs/master/kyma/docs/assets/","docsType":"root","topic":"kyma","slidesBanner":{"bannerDuration":5000,"slides":[{"text":"Don't miss the session by Piotr Kopczynski at Helm Summit on September 11 at 15:47.","url":"https://helmsummit2019.sched.com/event/S8sS","startDate":"09/09/2019","endDate":"12/09/2019"}]},"locale":"en"}}}