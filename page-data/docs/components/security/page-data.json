{"componentChunkName":"component---src-views-docs-index-tsx","path":"/docs/components/security","webpackCompilationHash":"aefb7e842b40c338951f","result":{"pageContext":{"isCreatedByStatefulCreatePages":false,"version":"","versions":{"releases":["0.9"],"prereleases":["1.0"],"branches":["master"]},"content":{"id":"security","displayName":"Security","description":"Overall documentation for Security","type":"components","docs":[{"order":"01-01-security","title":"Overview","source":"\nThe security model in Kyma uses the Service Mesh component to enforce authorization through [Kubernetes Role Based Authentication](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) (RBAC) in the cluster. The identity federation is managed through [Dex](https://github.com/coreos/dex), which is an open-source, OpenID Connect identity provider.\n\nDex implements a system of connectors that allow you to delegate authentication to external OpenID Connect and SAML2-compliant Identity Providers and use their user stores. Read [this](#details-add-an-identity-provider-to-dex) document to learn how to enable authentication with an external Identity Provider by using a Dex connector.\n\nOut of the box, Kyma comes with its own static user store used by Dex to authenticate users. This solution is designed for use with local Kyma deployments as it allows to easily create predefined users' credentials by creating Secret objects with a custom `dex-user-config` label.\nRead [this](#details-manage-static-users-in-dex) document to learn how to manage users in the static store used by Dex.\n\nKyma uses a group-based approach to managing authorizations.\nTo give users that belong to a group access to resources in Kyma, you must create:\n\n- Role and RoleBinding - for resources in a given Namespace.\n- ClusterRole and ClusterRoleBinding - for resources available in the entire cluster.\n\nThe RoleBinding or ClusterRoleBinding must have a group specified as their subject.\nSee [this](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) document to learn how to manage Roles and RoleBindings.\n\n>**NOTE:** You cannot define groups for the static user store. Instead, bind the user directly to a role or a cluster role by setting the user as the subject of a RoleBinding or ClusterRoleBinding.\n\nThe system creates two default roles in every Namespace:\n- `kyma-admin-role` - this role gives the user full access to the Namespace.\n- `kyma-reader-role` - this role gives the user the right to read all resources in the given Namespace.\n\nFor more details about Namespaces, read [this](/docs/root/kyma#details-namespaces) document.\n\n>**NOTE:** The **Global permissions** section in the **Administration** view of the Kyma Console UI allows you to manage user-group bindings.\n"},{"order":"02-01-security","title":"Architecture","source":"\nThe following diagram illustrates the authorization and authentication flow in Kyma. The representation assumes the Kyma Console UI as the user's point of entry.\n\n![authorization-authentication-flow](./assets/001-kyma-authorization.svg)\n\n1. The user opens the Kyma Console UI. If the Console application doesn't find a JWT token in the browser session storage, it redirects the user's browser to the Open ID Connect (OIDC) provider, Dex.\n2. Dex lists all defined Identity Provider connectors to the user. The user selects the Identity Provider to authenticate with. After successful authentication, the browser is redirected back to the OIDC provider which issues a JWT token to the user. After obtaining the token, the browser is redirected back to the Console UI. The Console UI stores the token in the Session Storage and uses it for all subsequent requests.\n3. The Authorization Proxy validates the JWT token passed in the **Authorization Bearer** request header. It extracts the user and groups details, the requested resource path, and the request method from the token. The Proxy uses this data to build an attributes record, which it sends to the Kubernetes Authorization API.\n4. The Proxy sends the attributes record to the Kubernetes Authorization API. If the authorization fails, the flow ends with a `403` code response.\n5. If the authorization succeeds, the request is forwarded to the Kubernetes API Server.  \n\n>**NOTE:** The Authorization Proxy can verify JWT tokens issued by Dex because Dex is registered as a trusted issuer through OIDC parameters during the Kyma installation.  \n"},{"order":"03-01-kubecofig-generator","title":"Kubeconfig generator","source":"\nThe Kubeconfig generator is a proprietary tool that generates a `kubeconfig` file which allows the user to access the Kyma cluster through the Command Line Interface (CLI), and to manage the connected cluster within the permission boundaries of the user.\n\nThe Kubeconfig generator rewrites the ID token issued for the user by Dex into the generated `kubeconfig` file. The time to live (TTL) of the ID token is 8 hours, which effectively means that the TTL of the generated `kubeconfig` file is 8 hours as well.\n\nThe generator is a publicly exposed service. You can access it directly under the `https://configurations-generator.{YOUR_CLUSTER_DOMAIN}` address. The service requires a valid ID token issued by Dex to return a code `200` result.\n\n## Get the kubeconfig file and configure the CLI\n\nFollow these steps to get the `kubeconfig` file and configure the CLI to connect to the cluster:\n\n1. Access the Console UI of your Kyma cluster.\n2. Click **Administration**.\n3. Click the **Download config** button to download the `kubeconfig` file to a selected location on your machine.\n4. Open a terminal window.\n5. Export the **KUBECONFIG** environment variable to point to the downloaded `kubeconfig`. Run this command:\n  ```\n  export KUBECONFIG={KUBECONFIG_FILE_PATH}\n  ```\n  >**NOTE:** Drag and drop the `kubeconfig` file in the terminal to easily add the path of the file to the `export KUBECONFIG` command you run.\n\n6. Run `kubectl cluster-info` to check if the CLI is connected to the correct cluster.\n\n>**NOTE:** Exporting the **KUBECONFIG** environment variable works only in the context of the given terminal window. If you close the window in which you exported the variable, or if you switch to a new terminal window, you must export the environment variable again to connect the CLI to the desired cluster.\n\nAlternatively, get the `kubeconfig` file by sending a `GET` request with a valid ID token issued for the user to the `/kube-config` endpoint of the `https://configurations-generator.{YOUR_CLUSTER_DOMAIN}` service. For example:\n```\ncurl GET https://configurations-generator.{YOUR_CLUSTER_DOMAIN}/kube-config -H \"Authorization: Bearer {VALID_ID_TOKEN}\"\n```\n","type":"Details"},{"order":"03-02-graphql","title":"GraphQL","source":"\nKyma uses a custom [GraphQL](https://graphql.org/) implementation in the Console Backend Service and deploys an RBAC-based logic to control the access to the GraphQL endpoint. All calls to the GraphQL endpoint require a valid Kyma token for authentication.\n\nThe authorization in GraphQL uses RBAC, which means that:\n  - All of the Roles, RoleBindings, ClusterRoles and CluserRoleBindings that you create and assign are effective and give the same permissions when users interact with the cluster resources both through the CLI and the GraphQL endpoints.\n  - To give users access to specific queries you must create appropriate Roles and bindings in your cluster.\n\nThe implementation assigns GraphQL actions to specific Kubernetes verbs:\n\n| GraphQL action | Kubernetes verb(s) |\n|:---|:---|\n| **query** | get (for a single resource) <br> list (for multiple resources) |\n| **mutation** | create, delete |\n| **subscription** | watch |\n\n> **NOTE:** Due to the nature of Kubernetes, you can secure specific resources specified by their name only for queries and mutations. Subscriptions work only with entire resource groups, such as kinds, and therefore don't allow for such level of granularity.\n\n## Available GraphQL actions\n\nTo access cluster resources through GraphQL, an action securing given resource must be defined and implemented in the cluster.\nSee the [GraphQL schema](https://github.com/kyma-project/kyma/blob/master/components/console-backend-service/internal/gqlschema/schema.graphql) file for the list of actions implemented in every Kyma cluster by default.\n\n## Secure a defined GraphQL action\n\nThis is an example GraphQL action implemented in Kyma out of the box.\n\n  ```\n  IDPPreset(name: String!): IDPPreset @HasAccess(attributes: {resource: \"IDPPreset\", verb: \"get\", apiGroup: \"authentication.kyma-project.io\", apiVersion: \"v1alpha1\"})\n  ```\n\nThis query secures the access to [IDPPreset](#custom-resource-idppreset) custom resources with specific names. To access it, the user must be bound to a role that allows to access:\n  - resources of the IDPPreset kind\n  - the Kubernetes verb `get`\n  - the `authentication.kyma-project.io` apiGroup\n\n\nTo allow access specifically to the example query, create this RBAC role in the cluster and bind it to a user or a client:\n\n  ```\n  apiVersion: rbac.authorization.k8s.io/v1beta1\n  kind: Role\n  metadata:\n    name: kyma-idpp-query-example\n  rules:\n  - apiGroups: [\"authentication.kyma-project.io\"]\n    resources: [\"idppresets\"]\n    verbs: [\"get\"]\n  ```\n\n> **NOTE:** To learn more about RBAC authorization in a Kubernetes cluster, read [this](https://kubernetes.io/docs/reference/access-authn-authz/rbac/) document.\n","type":"Details"},{"order":"03-03-graphql-request-flow","title":"GraphQL request flow","source":"\nThis diagram illustrates the request flow for the Console Backend Service which uses a custom [GraphQL](https://graphql.org/) implementation:\n\n![GraphQL request flow](./assets/002-graphql-request-flow.svg)\n\n1. The user sends a request with an ID token to the GraphQL application.\n2. The GraphQL application validates the user token and extracts user data required to perform [Subject Access Review](https://kubernetes.io/docs/reference/access-authn-authz/authorization/#checking-api-access) (SAR).\n3. The [Kubernetes API Server](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/) performs SAR.\n4. Based on the results of SAR, the Kubernetes API Server informs the GraphQL application whether the user can perform the requested [GraphQL action](#details-graphql-available-graphql-actions).\n5. Based on the information provided by the Kubernetes API Server, the GraphQL application returns an appropriate response to the user.\n\n>**NOTE:** Read [this](#details-graphql) document to learn more about the custom GraphQL implementation in Kyma.\n","type":"Details"},{"order":"03-03-tiller-tls","title":"TLS in Tiller","source":"\nKyma comes with a custom installation of [Tiller](https://helm.sh/docs/glossary/#tiller) which secures all incoming traffic with TLS certificate verification. To enable communication with Tiller, you must save the client certificate, key, and the cluster Certificate Authority (CA) to [Helm Home](https://helm.sh/docs/glossary/#helm-home-helm-home). \n\nSaving the client certificate, key, and CA to [Helm Home](https://helm.sh/docs/glossary/#helm-home-helm-home) is manual on cluster deployments. When you install Kyma locally, this process is handled by the `run.sh` script. \n\nAdditionally, you must add the `--tls` flag to every Helm command. \nIf you don't save the required certificates in Helm Home, or you don't include the `--tls` flag when you run a Helm command, you get this error: \n```\nError: transport is closing\n```\n\n## Add certificates to Helm Home\n\nTo get the client certificate, key, and the cluster CA and add them to [Helm Home](https://helm.sh/docs/glossary/#helm-home-helm-home), run these commands: \n  ```bash\n  kubectl get -n kyma-installer secret helm-secret -o jsonpath=\"{.data['global\\.helm\\.ca\\.crt']}\" | base64 --decode > \"$(helm home)/ca.pem\";\n  kubectl get -n kyma-installer secret helm-secret -o jsonpath=\"{.data['global\\.helm\\.tls\\.crt']}\" | base64 --decode > \"$(helm home)/cert.pem\";\n  kubectl get -n kyma-installer secret helm-secret -o jsonpath=\"{.data['global\\.helm\\.tls\\.key']}\" | base64 --decode > \"$(helm home)/key.pem\";\n  ```\n\n> **CAUTION:** All certificates are saved to Helm Home under the same, default path. When you save certificates of multiple clusters to Helm Home, one set of certificates overwrites the ones that already exist in Helm Home. As a result, you must save the cluster certificate set to Helm Home every time you switch the cluster context you work in. \n\n## Development\n\nTo connect to the Tiller server, for example, using the [Helm GO library](https://godoc.org/k8s.io/helm/pkg/helm#pkg-index), mount the Helm client certificates into the application you want to connect. These certificates are stored as a Kubernertes Secret. \n\nTo get this Secret, run: \n  ```bash\n  kubectl get secret -n kyma-installer helm-secret                            \n  ```\n\nAdditionally, those secrets are also available as overrides during Kyma installation:\n\n| Override | Description |\n| --- | --- | \n| **global.helm.ca.crt** | Certificate Authority for the Helm client |\n| **global.helm.tls.crt** | Client certificate for the Helm client | \n| **global.helm.tls.key** | Client certificate key for the Helm client |  \n","type":"Details"},{"order":"06-01-group","title":"Group","source":"\nThe `groups.authentication.kyma-project.io` CustomResourceDefinition (CRD) is a detailed description of the kind of data and the format that represents user groups available in the ID provider in the Kyma cluster. To get the up-to-date CRD and show the output in the `yaml` format, run this command:\n\n```\nkubectl get crd groups.authentication.kyma-project.io -o yaml\n```\n\n## Sample custom resource\n\nThis is a sample CR that represents an user group available in the ID provider in the Kyma cluster.\n\n```\napiVersion: authentication.kyma-project.io/v1alpha1\nkind: Group\nmetadata:\n    name: \"sample-group\"\nspec:    \n    name: \"admins\"\n    idpName: \"github\"\n    description: \"'admins' represents the group of users with administrative privileges in the organization.\"\n```\n\nThis table analyses the elements of the sample CR and the information it contains:\n\n\n| Field   |      Mandatory      |  Description |\n|:----------:|:-------------:|:------|\n| **metadata.name** |    **YES**   | Specifies the name of the CR. |\n| **spec.name** | **YES** | Specifies the name of the group. |\n| **spec.idpName** | **YES** | Specifies the name of the ID provider in which the group exists. |\n| **spec.description** | **NO** | Description of the group available in the ID provider. |\n","type":"Custom Resource"},{"order":"06-02-idppreset","title":"IDPPreset","source":"\nThe `idppresets.authentication.kyma-project.io` CustomResourceDefinition (CRD) is a detailed description of the kind of data and the format that represents presets of the Identity Provider configuration used to secure API through the Console UI. Presets are a convenient way to configure the **authentication** section in the API custom resource.\n\nTo get the up-to-date CRD and show the output in the `yaml` format, run this command:\n\n```bash\nkubectl get crd idppresets.authentication.kyma-project.io -o yaml\n```\n\n## Sample custom resource\n\nThis is a sample CR used to create an IDPPreset:\n\n```yaml\napiVersion: authentication.kyma-project.io/v1alpha1\nkind: IDPPreset\nmetadata:\n    name: \"sample-idppreset\"\nspec:\n    issuer: https://example.com\n    jwksUri: https://example.com/keys\n```\n\n## Custom resource parameters\n\nThis table lists all the possible parameters of a given resource together with their descriptions:\n\n| Parameter   |      Mandatory      |  Description |\n|:----------:|:-------------:|:------|\n| **metadata.name** |    **YES**   | Specifies the name of the CR. |\n| **spec.issuer** | **YES** | Specifies the issuer of the JWT tokens used to access the services. |\n| **spec.jwksUri** | **YES** | Specifies the URL of the OpenID Provider’s public key set to validate the signature of the JWT token. |\n\n## Usage in the UI\n\n**issuer** and **jwksUri** are some of the API CR specification fields. In most cases, these values are reused many times. IDPPresets usage is a solution to reuse them in a convenient way. It allows you to choose a proper preset from the dropdown menu instead of entering them manually every time you expose a secured API. Apart from consuming the IDPPresets, you can also manage them in the Console UI. To create and delete IDPPresets, go to the **Administration** tab and then to **IDP Presets**.\n\n## Related resources and components\n\nThese components use this CR:\n\n| Name   |   Description |\n|:----------:|:------|\n| IDP Preset |  Generates Go client which allows components and tests to create, delete, or get IDP Preset resources. |\n| Console Backend Service |  Enables the IDP Preset management with GraphQL API. |\n","type":"Custom Resource"},{"order":"08-01-update-tls-certificates","title":"Update TLS certificate","source":"\nThe TLS certificate is a vital security element. Follow this tutorial to update the TLS certificate in Kyma.\n\n>**NOTE:** This procedure can interrupt the communication between your cluster and the outside world for a limited\nperiod of time.\n\n## Prerequisites\n * New TLS certificates\n * Kyma administrator access\n\n## Steps\n\n1. Export the new TLS certificate and key as environment variables. Run:\n\n    ```bash\n    export KYMA_TLS_CERT=$(cat {NEW_CERT_PATH})\n    export KYMA_TLS_KEY=$(cat {NEW_KEY_PATH})\n    ```\n\n2. Update the Ingress Gateway certificate. Run:\n\n    ```bash\n    cat <<EOF | kubectl apply -f -\n    apiVersion: v1\n    kind: Secret\n    type: kubernetes.io/tls\n    metadata:\n        name: istio-ingressgateway-certs\n        namespace: istio-system\n    data:\n        tls.crt: $(echo \"$KYMA_TLS_CERT\" | base64)\n        tls.key: $(echo \"$KYMA_TLS_KEY\" | base64)\n    EOF\n    ```\n\n3. Update the `kyma-system` Namespace certificate:\n\n    ```bash\n    cat <<EOF | kubectl apply -f -\n    apiVersion: v1\n    kind: Secret\n    type: Opaque\n    metadata:\n        name: ingress-tls-cert\n        namespace: kyma-system\n    data:\n        tls.crt: $(echo \"$KYMA_TLS_CERT\" | base64)\n    EOF\n    ```\n\n4. Update the `kyma-integration` Namespace certificate:\n\n    ```bash\n    cat <<EOF | kubectl apply -f -\n    apiVersion: v1\n    kind: Secret\n    type: Opaque\n    metadata:\n        name: ingress-tls-cert\n        namespace: kyma-integration\n    data:\n        tls.crt: $(echo \"$KYMA_TLS_CERT\" | base64)\n    EOF\n    ```\n\n5. Restart the Ingress Gateway Pod to apply the new certificate:\n\n    ```bash\n    kubectl delete pod -l app=istio-ingressgateway -n istio-system\n    ```\n\n6. Restart the Pods in the `kyma-system` Namespace to apply the new certificate:\n\n    ```bash\n    kubectl delete pod -l tlsSecret=ingress-tls-cert -n kyma-system\n    ```\n\n7. Restart the Pods in the `kyma-integration` Namespace to apply the new certificate:\n\n    ```bash\n    kubectl delete pod -l tlsSecret=ingress-tls-cert -n kyma-integration\n    ```\n","type":"Tutorials"},{"order":"08-02-manage-static-users","title":"Manage static users in Dex","source":"\n## Create a new static user\n\nTo create a static user in Dex, create a Secret with the **dex-user-config** label set to `true`. Run:\n\n```\ncat <<EOF | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n  name:  {SECRET_NAME}\n  namespace: {SECRET_NAMESPACE}\n  labels:\n    \"dex-user-config\": \"true\"\ndata:\n  email: {BASE64_USER_EMAIL}\n  username: {BASE64_USERNAME}\n  password: {BASE64_USER_PASSWORD}  \ntype: Opaque\nEOF\n```\n>**NOTE:** If you don't specify the Namespace in which you want to create the Secret, the system creates it in the `default` Namespace.\n\nThe following table describes the fields that are mandatory to create a static user. If you don't include any of these fields, the user is not created.\n\n|Field | Description |\n|---|---|\n| data.email | Base64-encoded email address used to sign-in to the console UI. Must be unique. |\n| data.username | Base64-encoded username displayed in the console UI. |\n| data.password | Base64-encoded user password. There are no specific requirements regarding password strength, but it is recommended to use a password that is at least 8-characters-long. |\n\nCreate the Secrets in the cluster before Dex is installed. The Dex init-container with the tool that configures Dex generates user configuration data basing on properly labeled Secrets, and adds the data to the ConfigMap.\n\nIf you want to add a new static user after Dex is installed, restart the Dex Pod. This creates a new Pod with an updated ConfigMap.\n\n## Bind a user to a Role or a ClusterRole\n\nA newly created static user has no access to any resources of the cluster as there is no Role or ClusterRole bound to it.  \nBy default, Kyma comes with the following ClusterRoles:\n \n- **kyma-admin**: allows full admin access to the entire cluster\n- **kyma-edit**: allows full access to all Kyma-managed resources\n- **kyma-developer**:  allows full access to Kyma-managed resources and basic Kubernetes resources\n- **kyma-view**: allows to view and list all of the resources of the cluster\n- **kyma-essentials**: set of  minimal view access right to use the kyma console\n\nTo bind a newly created user to the **kyma-view** ClusterRole, run this command:\n```\nkubectl create clusterrolebinding {BINDING_NAME} --clusterrole=kyma-view --user={USER_EMAIL}\n```\n\nTo check if the binding is created, run:\n```\nkubectl get clusterrolebinding {BINDING_NAME}\n```\n","type":"Tutorials"},{"order":"08-03-add-connector","title":"Add an Identity Provider to Dex","source":"\nAdd external, OpenID Connect compliant authentication providers to Kyma using [Dex connectors](https://github.com/coreos/dex#connectors). Follow this tutorial to add a GitHub connector and use it to authenticate users in Kyma.\n\n>**NOTE:** Groups in the Github are represented as teams. See [this](https://help.github.com/articles/organizing-members-into-teams/) document to learn how to manage teams in Github.\n\n## Prerequisites\n\nTo add a GitHub connector to Dex, [register](https://github.com/settings/applications/new) a new OAuth application in GitHub. Set the authorization callback URL to `https://dex.kyma.local/callback`.\nAfter you complete the registration, [request](https://help.github.com/articles/requesting-organization-approval-for-oauth-apps/) for an organization approval.\n\n>**NOTE:** To authenticate in Kyma using GitHub, the user must be a member of a GitHub [organization](https://help.github.com/articles/creating-a-new-organization-from-scratch/) that has at least one [team](https://help.github.com/articles/creating-a-team/).\n\n## Configure Dex\n\nRegister the GitHub Dex connector by editing the `dex-config-map.yaml` ConfigMap file located in the `kyma/resources/dex/templates` directory. Follow this template:\n\n```\n    connectors:\n    - type: github\n      id: github\n      name: GitHub\n      config:\n        clientID: {GITHUB_CLIENT_ID}\n        clientSecret: {GITHUB_CLIENT_SECRET}\n        redirectURI: https://dex.kyma.local/callback\n        orgs:\n          - name: {GITHUB_ORGANIZATION}\n```\n\nThis table explains the placeholders used in the template:\n\n|Placeholder | Description |\n|---|---|\n| GITHUB_CLIENT_ID | Specifies the application's client ID. |\n| GITHUB_CLIENT_SECRET | Specifies the application's client Secret. |\n| GITHUB_ORGANIZATION | Specifies the name of the GitHub organization. |\n\n## Configure authorization rules\n\nTo bind Github groups to the default roles added to every Kyma Namespace, add the **bindings** section to [this](https://github.com/kyma-project/kyma/blob/master/resources/core/charts/cluster-users/values.yaml) file. Follow this template:\n\n```\nbindings:\n  kymaAdmin:\n    groups:\n    - \"{GITHUB_ORGANIZATION}:{GITHUB_TEAM_A}\"\n  kymaView:\n    groups:\n    - \"{GITHUB_ORGANIZATION}:{GITHUB_TEAM_B}\"\n```\n\nThis table explains the placeholders used in the template:\n\n|Placeholder | Description |\n|---|---|\n| GITHUB_ORGANIZATION | Specifies the name of the GitHub organization. |\n| GITHUB_TEAM_A | Specifies the name of GitHub team to bind to the `kyma-admin-role` role. |\n| GITHUB_TEAM_B | Specifies the name of GitHub team to bind to the `kyma-reader-role` role. |\n","type":"Tutorials"}]},"navigation":{"root":[{"displayName":"Kyma","id":"kyma"}],"components":[{"displayName":"Security","id":"security"},{"displayName":"Service Catalog","id":"service-catalog"},{"displayName":"Helm Broker","id":"helm-broker"},{"displayName":"Application Connector","id":"application-connector"},{"displayName":"Event Bus","id":"event-bus"},{"displayName":"Service Mesh","id":"service-mesh"},{"displayName":"Serverless","id":"serverless"},{"displayName":"Monitoring","id":"monitoring"},{"displayName":"Tracing","id":"tracing"},{"displayName":"API Gateway","id":"api-gateway"},{"displayName":"Logging","id":"logging"},{"displayName":"Backup","id":"backup"},{"displayName":"Console","id":"console"},{"displayName":"Asset Store","id":"asset-store"},{"displayName":"Headless CMS","id":"headless-cms"}]},"manifest":{"root":[{"displayName":"Kyma","id":"kyma"}],"components":[{"displayName":"Security","id":"security"},{"displayName":"Service Catalog","id":"service-catalog"},{"displayName":"Helm Broker","id":"helm-broker"},{"displayName":"Application Connector","id":"application-connector"},{"displayName":"Event Bus","id":"event-bus"},{"displayName":"Service Mesh","id":"service-mesh"},{"displayName":"Serverless","id":"serverless"},{"displayName":"Monitoring","id":"monitoring"},{"displayName":"Tracing","id":"tracing"},{"displayName":"API Gateway","id":"api-gateway"},{"displayName":"Logging","id":"logging"},{"displayName":"Backup","id":"backup"},{"displayName":"Console","id":"console"},{"displayName":"Asset Store","id":"asset-store"},{"displayName":"Headless CMS","id":"headless-cms"}]},"assetsPath":"/assets/docs/0.9/security/docs/assets/","docsType":"components","topic":"security","slidesBanner":{"bannerDuration":5000,"slides":[{"text":"Don't miss the session by Piotr Kopczynski at Helm Summit on September 11 at 15:47.","url":"https://helmsummit2019.sched.com/event/S8sS","startDate":"09/09/2019","endDate":"12/09/2019"}]},"locale":"en"}}}