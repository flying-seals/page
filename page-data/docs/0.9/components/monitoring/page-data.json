{"componentChunkName":"component---src-views-docs-index-tsx","path":"/docs/0.9/components/monitoring","webpackCompilationHash":"aefb7e842b40c338951f","result":{"pageContext":{"isCreatedByStatefulCreatePages":false,"version":"0.9","versions":{"releases":["0.9"],"prereleases":["1.0"],"branches":["master"]},"content":{"id":"monitoring","displayName":"Monitoring","description":"Overall documentation for Monitoring","type":"components","docs":[{"order":"01-01-monitoring","title":"Overview","source":"\nTo enrich Kyma with monitoring functionality, third-party resources come by default as packaged tools. The `kube-prometheus` package is a Prometheus operator from CoreOS responsible for delivering these tools. Monitoring in Kyma includes three primary elements:\n\n* Prometheus, an open-source system monitoring toolkit.\n* Grafana, a user interface that allows you to query and visualize statistics and metrics.  \n* AlertManager, a Prometheus component that handles alerts that originate from Prometheus. AlertManager performs needed deduplicating, grouping, and routing based on rules defined by the Prometheus server.\n\nConvenience and efficiency are the main advantages to using the `kube-prometheus` package. `kube-prometheus` delivers a level of monitoring options that would otherwise involve extensive development effort to acquire. Prometheus, Grafana, and AlertManager installed on their own would require the developer to perform customization to achieve the same results as the operator alone. `kube-prometheus` is configured to run on Kubernetes and monitor clusters without additional configuration.\n"},{"order":"02-01-monitoring","title":"Architecture","source":"\nThis document outlines the monitoring architecture of Kyma, highlighting information sources that Prometheus polls for data to process.\n\n\n![Monitoring architecture in Kyma](./assets/monitoring.png)\n\n\n## The Prometheus Operator\n\nThe Prometheus Operator is a CoreOS component integrated into Kyma that enables Prometheus deployments to be decoupled from the configuration of the entities they monitor. The task of the Operator is to ensure that Prometheus servers with the specified configuration are always running. If the developer does not specify a configuration for Prometheus instances, the Operator is able to generate and deploy one. The Prometheus instance is responsible for the monitoring of services.\n\n## The Service Monitor\n\nThe Service Monitor works in orchestration with the Prometheus resource that the Operator watches. It dictates to a Prometheus resource how to retrieve metrics and enables exposure of those metrics in a standard manner. It also specifies services the Prometheus instance should monitor. Using labels, the Prometheus resource includes a Service Monitor.  \n\n## Monitored Data sources\n\nPrometheus contains the flexibility to poll data from a variety of sources. Virtual machines on which Kubernetes runs make time-stamped data available, reporting on jobs started, workload, CPU performance, capacity, and more. In this case, the Service Monitor watches the Kubernetes API master to detect any job creation. The job produces time-stamped data that Prometheus consumes.\n\nPods may contain applications with custom metrics that Prometheus can poll through the Prometheus exporter.\n\n## Grafana\n\nKyma employs Grafana as a third-party resource in `kube-prometheus` to deliver a feature-rich metrics dashboard and graph editor.\n\nTo access the Grafana UI, use the following URL: `https://grafana.{DOMAIN}`. Replace DOMAIN with the domain of your Kyma cluster.\n\n## Alertmanager\n\nAlertmanager receives harvested metrics from Prometheus and forwards this data on to the configured channels, such as email or incident management systems.\n"},{"order":"08-01-monitoring-custom-metrics","title":"Expose custom metrics in Kyma","source":"\nThis tutorial shows how to expose custom metrics to Prometheus with a Golang service in Kyma. To do so, follow these steps:\n\n1. Configure Istio.\n2. Expose a sample application serving metrics on `8081` port.\n3. Access the exposed metrics in Prometheus.\n\n## Prerequisites\n\n- Kyma as the target deployment environment\n- Istio 0.8\n  - sidecar injection enabled\n  - mutual TLS enabled\n\n## Installation\n\n### Configure Istio\n\nFor the `default` Namespace, you must enable the sidecar injection. To enable the sidecar injection for all Pods in the `default` Namespace, run the following command:\n\n```bash\nkubectl label namespace default istio-injection=enabled\nnamespace \"default\" labeled\n```\n\nFor more details on deploying your application with Istio, read [this](https://istio.io/docs/setup/kubernetes/install/) documentation.\n\nYou must also add the **sidecar.istio.io/inject** annotation with the value set to `true` to the Pod template specification, to enable the injection as shown in [this](https://github.com/kyma-project/examples/blob/master/monitoring-custom-metrics/deployment/deployment.yaml#L12) example.\n\n```yaml\nspec:\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: \"true\"\n```\n\nFor more details on installing the Istio sidecar, read [this](https://istio.io/docs/setup/kubernetes/additional-setup/sidecar-injection/) documentation.\n\nThe following ports are used in the Pod:\n\n- `8080` - Envoy captures the traffic only for ports listed in Pod's **containerPorts** (`containerPort: 8080`), or in the **traffic.sidecar.istio.io/includeInboundPorts** annotation. Thus, this port is a part of the Service Mesh and can be used for application's needs.\n\n- `8081` - This is the excluded port from the Service Mesh, which is used for exposing metrics only. The network traffic bypasses Envoy and goes straight to the container. In Kyma, use the suggested `8081` port to expose metrics.\n\n### Expose a sample metrics application\n\nTo expose Prometheus metrics in Golang, the Prometheus community provides [this](https://github.com/prometheus/client_golang) library.\n\nThis is a basic example where `Gauge` and `Counter` metrics are exported using the `prometheus` package.\n\n1. Deploy the sample metrics application.\n    ```bash\n    kubectl apply -f https://raw.githubusercontent.com/kyma-project/examples/master/monitoring-custom-metrics/deployment/deployment.yaml\n\n    kubectl apply -f https://raw.githubusercontent.com/kyma-project/examples/master/monitoring-custom-metrics/deployment/service-monitor.yaml\n    ```\n\n    ```bash\n    kubectl get pods\n    NAME                             READY     STATUS    RESTARTS   AGE\n    sample-metrics-67c6885d8c-smt62   2/2       Running   0          2m\n    ```\n\n2. Run the `port-forward` command on the `sample-metrics-8081` service for the`8081` port to check the metrics.\n    ```bash\n    kubectl port-forward svc/sample-metrics-8081 8081:8081\n    ```\n    Open a browser and access [`http://localhost:8081/metrics`](http://localhost:8081/metrics)\n\n    ![metrics on port 8081](./assets/sample-metrics-2.png)\n\nFind the source code for the sample application [here](https://github.com/kyma-project/examples/blob/master/monitoring-custom-metrics/main.go). See the [package prometheus](https://godoc.org/github.com/prometheus/client_golang/prometheus) for the reference documentation. Read [this](https://prometheus.io/docs/concepts/metric_types/) documentation to learn more about the Prometheus metric types.\n\n### Access the exposed metrics in Prometheus\n\n Run the `port-forward` command on the `monitoring-prometheus` service:\n\n```bash\nkubectl port-forward svc/monitoring-prometheus -n kyma-system 9090:9090\nForwarding from 127.0.0.1:9090 -> 9090\nForwarding from [::1]:9090 -> 9090\n```\n\nAll the **sample-metrics** endpoints appear as the [`Targets`](http://localhost:9090/targets#job-sample-metrics-8081) list.\n\n![Prometheus Dashboard](./assets/pm-dashboard-1.png)\n\nUse either the `cpu_temperature_celsius` or `hd_errors_total` in the [**expression**](http://localhost:9090/graph) field. Click the **Execute** button to check the values scrapped by Prometheus.\n\n![Prometheus Dashboard](./assets/pm-dashboard-2.png)\n\n### ServiceMonitor\n\nPrometheus can reach the service using ServiceMonitor. ServiceMonitor is a specific CRD used by the Prometheus operator to monitor services.\n\nIn Kyma, the Prometheus server discovers all ServiceMonitors through the **serviceMonitorSelector** matching the `prometheus: monitoring` label.\n\n```yaml\n  serviceMonitorSelector:\n    matchLabels:\n      prometheus: {{ .Values.prometheusLabelValue | default .Release.Name | quote }}\n{{- end }}\n```\n\nIn this example, the ServiceMonitor selects a **selector** with all services matching the `k8s-app: metrics` label. Find the complete yaml [here](https://github.com/kyma-project/examples/blob/master/monitoring-custom-metrics/deployment/service-monitor.yaml).\n\nIn Kyma, there is a [template](https://github.com/kyma-project/kyma/blob/master/resources/monitoring/charts/prometheus/templates/servicemonitors.yaml) which serves to discover a list of ServiceMonitors.\n\n### Add a Custom Dashboard in Grafana\n\nOut of the box, Kyma includes a set of dashboards. The users can create their own **Grafana Dashboard** by using the Grafana UI. The dashboards persist even after the Pod restarts.\n\nFor details on how to create dashboards in Grafana, see the following documents:\n\n- [Grafana in Kyma](https://github.com/kyma-project/kyma/blob/master/resources/monitoring/charts/grafana/README.md)\n- [Grafana - Getting started](http://docs.grafana.org/guides/getting_started/)\n- [Export and Import dashboards](http://docs.grafana.org/reference/export_import/)\n- [Grafana - Dashboard API](http://docs.grafana.org/http_api/dashboard/)\n\n### Cleanup\n\nRun the following commands to completely remove the example and all its resources from the cluster:\n\n1. Remove the **istio-injection** label from the `default` Namespace.\n    ```bash\n    kubectl label namespace default istio-injection-\n    ```\n2. Remove **ServiceMonitor** in the `kyma-system` Namespace.\n    ```bash\n    kubectl delete servicemonitor -l example=monitoring-custom-metrics -n kyma-system\n    ```\n3. Remove the `sample-metrics` Deployments in the `default` Namespace.\n    ```bash\n    kubectl delete all -l example=monitoring-custom-metrics\n","type":"Tutorials"}]},"navigation":{"root":[{"displayName":"Kyma","id":"kyma"}],"components":[{"displayName":"Security","id":"security"},{"displayName":"Service Catalog","id":"service-catalog"},{"displayName":"Helm Broker","id":"helm-broker"},{"displayName":"Application Connector","id":"application-connector"},{"displayName":"Event Bus","id":"event-bus"},{"displayName":"Service Mesh","id":"service-mesh"},{"displayName":"Serverless","id":"serverless"},{"displayName":"Monitoring","id":"monitoring"},{"displayName":"Tracing","id":"tracing"},{"displayName":"API Gateway","id":"api-gateway"},{"displayName":"Logging","id":"logging"},{"displayName":"Backup","id":"backup"},{"displayName":"Console","id":"console"},{"displayName":"Asset Store","id":"asset-store"},{"displayName":"Headless CMS","id":"headless-cms"}]},"manifest":{"root":[{"displayName":"Kyma","id":"kyma"}],"components":[{"displayName":"Security","id":"security"},{"displayName":"Service Catalog","id":"service-catalog"},{"displayName":"Helm Broker","id":"helm-broker"},{"displayName":"Application Connector","id":"application-connector"},{"displayName":"Event Bus","id":"event-bus"},{"displayName":"Service Mesh","id":"service-mesh"},{"displayName":"Serverless","id":"serverless"},{"displayName":"Monitoring","id":"monitoring"},{"displayName":"Tracing","id":"tracing"},{"displayName":"API Gateway","id":"api-gateway"},{"displayName":"Logging","id":"logging"},{"displayName":"Backup","id":"backup"},{"displayName":"Console","id":"console"},{"displayName":"Asset Store","id":"asset-store"},{"displayName":"Headless CMS","id":"headless-cms"}]},"assetsPath":"/assets/docs/0.9/monitoring/docs/assets/","docsType":"components","topic":"monitoring","slidesBanner":{"bannerDuration":5000,"slides":[{"text":"Don't miss the session by Piotr Kopczynski at Helm Summit on September 11 at 15:47.","url":"https://helmsummit2019.sched.com/event/S8sS","startDate":"09/09/2019","endDate":"12/09/2019"}]},"locale":"en"}}}